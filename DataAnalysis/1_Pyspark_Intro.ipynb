{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### what is PySpark ?\n",
    "\n",
    "PySpark can be summarized as being the Python API to Spark\n",
    "\n",
    "### what is Spark ?\n",
    "\n",
    "It is a “unified analytics engine for large-scale data processing”. It can be described as an *analytics factory*. Spark can process an increasingly vast amount of data by scaling out (across multiple smaller machines) instead of scaling up (adding more resources, such as CPU, RAM, and disk space, to a single machine).\n",
    "\n",
    "**PySpark** is expressive\n",
    "```\n",
    "(\n",
    "    spark.read.csv(\"./data/list_of_numbers/sample.csv\", header=True)\n",
    "    .withColumn(\n",
    "        \"new_column\", F.when(F.col(\"old_column\") > 10, 10).otherwise(0)\n",
    "        )\n",
    "    .where(\"old_column > 8\")\n",
    "    .groupby(\"new_column\")\n",
    "    .count()\n",
    "    .write.csv(\"updated_frequencies.csv\", mode=\"overwrite\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How PySpark works\n",
    "\n",
    "![How spark works](images/how_spark_works.png)\n",
    "\n",
    "You have some workbenches that some workers are assigned to. The workbenches are like the computers in our Spark cluster: there is a fixed amount of them.  The workers are called executors in Spark’s literature: they perform the actual work on the machines/nodes. That top hat definitely makes him stand out from the crowd. In our data factory, he’s the manager of the work floor. In Spark terms, we call this the master. The master here sits on one of the workbenches/machines, but it can also sit on a distinct machine (or even your computer!) depending on the cluster manager and deployment mode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon reception of the task, which is called a driver program in the Spark world, the factory starts running. This doesn’t mean that we get straight to processing. Before that, the cluster needs to plan the capacity it will allocate for your program. The entity or program taking care of this is aptly called the cluster manager."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an *example*\n",
    "\n",
    "\\# Contents of the sample.csv\n",
    "```\n",
    "old_column\n",
    "1\n",
    "4\n",
    "4\n",
    "5\n",
    "7\n",
    "7\n",
    "7\n",
    "10\n",
    "14\n",
    "1\n",
    "4\n",
    "8\n",
    "```\n",
    "In the case of computing the average, each worker independently computes the sum of the values and their counts before moving the result — not all the data! —\n",
    "over to a single worker (or the master directly, when the intermediate result is really small) that will process the aggregation into a single number, the average.\n",
    "\n",
    "![Simple averaging of data](images/how_spark_works_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laziness: A Spark feature\n",
    "\n",
    "laziness is (in part) how Spark achieves its incredible processing speed. Just like in a large-scale factory, you don’t go to each employee and give them a\n",
    "list of tasks. No, here, the master/manager is responsible for the workers. The driver is where the action happens. Think of a driver as a floor lead: you provide them your list of steps and let them deal with it. In Spark, the driver/floor lead takes your instructions (carefully written in Python code), translates them into Spark steps, and then processes them across the worker. The driver also manages which worker/table has which slice of the data, and makes sure you don’t lose some bits in the process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- The master is like the factory owner, allocating resources as needed to complete\n",
    "the jobs.\n",
    "- The driver is responsible for completing a given job. It requests resources from\n",
    "the master as needed.\n",
    "- A worker is a set of computing/memory resources, like a workbench in our factory.\n",
    "- Executors sit atop a worker and perform the work sent by the driver, like\n",
    "employees at a workbench."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every instruction you’re providing in Spark can be classified into two categories: transformations and actions. Actions are what many programming languages would consider I/O. The most typical actions are the following:\n",
    "- Printing information on the screen\n",
    "- Writing data to a hard drive or cloud bucket\n",
    "- Counting the number of records\n",
    "\n",
    "In Spark, we’ll see those instructions most often via the show(), write(), and count() methods on a data frame. Transformations are pretty much everything else. Some examples of transformations\n",
    "are as follows:\n",
    "- Adding a column to a table\n",
    "- Performing an aggregation according to certain keys\n",
    "- Computing summary statistics\n",
    "- Training a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0b4 (main, Jul 11 2022, 15:47:56) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc4e11fd693f0ab1ad3a86ea90f3e32c51dc3a3204238a7053c4767532e2f698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
