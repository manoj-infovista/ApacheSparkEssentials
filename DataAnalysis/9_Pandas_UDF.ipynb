{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data: Using pandas UDFs\n",
    "\n",
    "PySpark’s interoperability with pandas (also colloquially called pandas UDF) is a huge selling point when performing data analysis at scale. pandas is the dominant in-memory Python data manipulation library, while PySpark is the dominantly distributed one. Combining both of them unlocks additional possibilities.\n",
    "\n",
    "We will look into operations on `GroupedData` and how PySpark plus Pandas implement the split-apply-combine pattern common to data analysis. We finish with the ultimate interaction between pandas and PySpark: treating a PySpark data frame like a small collection of pandas DataFrames."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column transformations with pandas: Using Series UDF\n",
    "\n",
    "The Series UDFs family shares a column-first focus with regular PySpark data transformation functions. All of our UDFs in this section will take a Column object (or objects) as input and return a Column object as output. \n",
    "\n",
    "PySpark provides three types of Series UDFs.\n",
    "- The *Series to Series* takes `Columns` objects as inputs, converts them to pandas Series objects, and returns a Series object that gets promoted back to a PySpark Column object.\n",
    "- The *Iterator of Series to Iterator of Series*  differs in the sense that the `Column` objects get batched into batches and then fed as Iterator objects. It takes a single Column object as input and returns a single `Column`.\n",
    "- The *Iterator of multiple Series to Iterator of Series* is a combination of the previous Series UDFs and can take multiple Columns as input, like the Series to Series UDF, yet preserves the iterator pattern from the Iterator of Series to Iterator of Series.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting Spark to Google's BigQuery\n",
    "\n",
    "We connect PySpark to Google’s BigQuery, where we will use the National Oceanic and Atmospheric Administration’s (NOAA) Global Surface Summary of the Day (GSOD) data set. In the same vein, this provides a blueprint for connecting PySpark to other data warehouses, such as SQL or NoSQL databases. \n",
    "\n",
    "You need a GCP account. Once your account is created, you need to create a service account and a service account key to tell BigQuery to give you access to the public data programmatically. To do so, select Service Account (under IAM & Admin) and click + Create Service Account. Give a meaningful name to your service account.\n",
    "In the service account permissions menu, select BigQuery → BigQuery admin and click\n",
    "Continue. In the last step, click + CREATE KEY and select JSON. Download the key and store it somewhere safe\n",
    "\n",
    "Download the Google's BigQuery connector from [here](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the connection betweek PySpark and BigQuery through connector\n",
    "\n",
    "We instruct Spark to fetch and install external dependencies, in our case, the `com.google.cloud.spark:spark-bigquery connector`. As it is a Java/Scala dependency, we need to match the correct Spark and Scala version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.jars.packages\",\n",
    "    \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.28.0\",\n",
    ").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data from BigQuery using our secret key\n",
    "\n",
    "we can start creating pandas UDFs: we just have to read the data. we assemble 10 years worth of weather data located in BigQuery, which totals over 40 million records.\n",
    "\n",
    "we use the bigqueryspecialized SparkReader—provided by the connector library we embedded to our PySpark shell—which provides two options:\n",
    "- The table parameter pointing to the table we want to ingest. The format is `project.dataset.table`; the `bigquery-public-data` is a project available to all.\n",
    "- The `credentialsFile` is the JSON key downloaded beffore. You need\n",
    "to adjust the path and file name according to the location of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def read_df_from_bq(year):\n",
    "    return (\n",
    "        spark.read.format(\"bigquery\").option(\n",
    "            \"table\", f\"bigquery-public-data.noaa_gsod.gsod{year}\"\n",
    "        )\n",
    "        .option(\"credentialsFile\", \"big-query-spark-key.json\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "\n",
    "gsod = (\n",
    "    reduce(\n",
    "        lambda x, y: x.unionByName(y, allowMissingColumns=True),\n",
    "        [read_df_from_bq(year) for year in range(2018, 2019)],\n",
    "    )\n",
    "    .dropna(subset=[\"year\", \"mo\", \"da\", \"temp\"])\n",
    "    .where(F.col(\"temp\") != 9999.9)\n",
    "    .drop(\"date\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series to Series UDF: Column functions, but with pandas\n",
    "\n",
    "The Series to Series UDF, also called Scalar UDF, are akin to most of the functions in the `pyspark.sql model`. For the most part, they work just like Python UDFs as seen in previous notebook, with one key difference: Python UDFs work on one record at a time, and you express your logic through regular Python code. Scalar UDFs work on one Series at a time, and you express your logic through pandas code. \n",
    "\n",
    "<img src=\"images/series_2_series_udf.png\" width=\"600px\">\n",
    "\n",
    "In a Python UDF, when you pass column objects to your UDF, PySpark will unpack\n",
    "each value, perform the computation, and then return the value for each record in\n",
    "a Column object. Whereas in a Scaler UDF, PySpark will serialize (through a library called PyArrow) each partitioned column into a pandas Series object. You then perform the operations on the Series object directly, returning a Series of\n",
    "the same dimension from your UDF.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple function that will transform Fahrenheit degrees to Celsius. \n",
    "- Instead of `udf()`, we use `pandas_udf()`, again, from the `pyspark.sql.functions` module. Optionally (but recommended), we can pass the return type of the UDF as an argument to the `pandas_udf()` decorator.\n",
    "- Our function signature is also different: rather than using scalar values (such as int or str), the UDF takes `pd.Series` and return a `pd.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def f_to_c(degrees: pd.Series) -> pd.Series:\n",
    "    \"\"\"Transforms Farhenheit to Celcius.\"\"\"\n",
    "    return (degrees - 32) * 5 / 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we apply our newly created Series to Series UDF to the temp column of the gsod data frame, which contains the temperature (in Fahrenheit) of each stationday combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|temp|             temp_c|\n",
      "+----+-------------------+\n",
      "|29.6|-1.3333333333333326|\n",
      "|53.5| 11.944444444444445|\n",
      "|71.6| 21.999999999999996|\n",
      "|70.4| 21.333333333333336|\n",
      "|37.2| 2.8888888888888906|\n",
      "+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod = gsod.withColumn(\"temp_c\", f_to_c(F.col(\"temp\")))\n",
    "gsod.select(\"temp\", \"temp_c\").distinct().show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar UDF + cold start = Iterator of Series UDF\n",
    "\n",
    "This section combines the other two types of Scalar UDFs: the *Iterator of Series to Iterator of Series* UDF and the *Iterator of multiple Series to Iterator of Series*. \n",
    "\n",
    "Iterator of Series UDFs are very useful when you have an expensive cold start operation you need to perform. By cold start, we mean an operation we need to perform once at the beginning of the processing step, before working through the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|temp|             temp_c|\n",
      "+----+-------------------+\n",
      "|29.6|-1.3333333333333326|\n",
      "|53.5| 11.944444444444445|\n",
      "|71.6| 21.999999999999996|\n",
      "|70.4| 21.333333333333336|\n",
      "|37.2| 2.8888888888888906|\n",
      "+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from typing import Iterator\n",
    "\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def f_to_c2(degrees: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Transforms Farhenheit to Celcius.\"\"\"\n",
    "    # We simulate a cold start using sleep() for five seconds. \n",
    "    # The cold start will happen on each worker\n",
    "    # once, rather than for every batch\n",
    "    sleep(5)\n",
    "     \n",
    "    for batch in degrees:\n",
    "        yield (batch - 32) * 5 / 9\n",
    "\n",
    "\n",
    "gsod.select(\n",
    "    \"temp\", f_to_c2(F.col(\"temp\")).alias(\"temp_c\")\n",
    ").distinct().show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Iterator of multiple Series to Iterator of Series is a special case to wrap multiple columns in a single iterator. We'll assemble the year, mo, and da columns\n",
    "(representing the year, month, and day) into a single column. This example requires more data transformation than when using an Iterator of a single Series.\n",
    "\n",
    "Our date assembly UDF works like this:\n",
    "1. year_mo_da is an Iterator of a tuple of Series, representing all the batches of values contained in the year, mo, and da columns.\n",
    "2. To access each batch, we use a for loop over the iterator, the same principle as for the Iterator of Series UDF.\n",
    "3. To extract each individual series from the tuple, we use multiple assignments.\n",
    "In this case, year will map to the first Series of the tuple, mo to the second, and da to the third.\n",
    "4. Since pd.to_datetime requests a data frame containing the year, month, and\n",
    "day columns, we create the data frame via a dictionary, giving the keys the relevant column names. pd.to_datetime returns a Series.\n",
    "5. Finally, we yield the answer to build the Iterator of Series, fulfilling our contract.\n",
    "\n",
    "<img src=\"images/iterator_of_mutl_series.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+----------+\n",
      "|year| mo| da|      date|\n",
      "+----+---+---+----------+\n",
      "|2018| 08| 21|2018-08-21|\n",
      "|2018| 07| 29|2018-07-29|\n",
      "|2018| 05| 12|2018-05-12|\n",
      "|2018| 03| 20|2018-03-20|\n",
      "|2018| 09| 11|2018-09-11|\n",
      "+----+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "@F.pandas_udf(T.DateType())\n",
    "def create_date(year_mo_da: Iterator[Tuple[pd.Series, pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Merges three cols (representing Y-M-D of a date) into a Date col.\"\"\"\n",
    "    for year, mo, da in year_mo_da:\n",
    "        yield pd.to_datetime(\n",
    "            pd.DataFrame(dict(year=year, month=mo, day=da))\n",
    "        )\n",
    "\n",
    "\n",
    "gsod.select(\n",
    "    \"year\", \"mo\", \"da\",\n",
    "    create_date(F.col(\"year\"), F.col(\"mo\"), F.col(\"da\")).alias(\"date\"),\n",
    ").distinct().show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar UDFs are very useful when you make column-level transformations, just like the functions in `pyspark.sql.functions`. When using any Scalar user-defined function, you need to remember that PySpark will not guarantee the order or the composition of the batches when applying it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDFs on grouped data: Aggregate and apply\n",
    "\n",
    "- *Group aggregate UDFs*: You need to perform aggregate functions such as `count()`\n",
    "or `sum()` as we saw in [Joing and Grouping](./5_Joining_Grouping.ipynb)\n",
    "- *Group map UDFs*: Your data frame can be split into batches based on the values\n",
    "of certain columns; you then apply a function on each batch as if it were a pandas `DataFrame` before combining each batch back into a Spark data frame. For instance, we could have our gsod data batched by station month and perform operations on the resulting data frames.\n",
    "\n",
    "Both group aggregate and group map UDFs are PySpark’s answer to the split-applycombine pattern. At the core, split-apply-combine is just a series of three steps that are frequently used in data analysis:\n",
    "1. Split your data set into logical batches (using `groupby()`).\n",
    "2. Apply a function to each batch independently.\n",
    "3. Combine the batches into a unified data set.\n",
    "\n",
    "<img src=\"images/split-apply-combine.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group aggregate UDFs\n",
    "\n",
    "The group aggregate UDF is also known as the *Series to Scalar UDF*. Unlike the *Series to Series*, the group aggregate UDF distills the Series received as input to a single value. PySpark provides the group aggregate functionality though the `groupby().agg()` pattern we saw in [Joining and Grouping](./5_Joining_Grouping.ipynb). A group aggregate UDF is simply a custom aggregate function we pass as an argument to `agg()`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exmaple, we compute the linear slope of the temperature for a given period using scikit-learn’s LinearRegression object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+--------------------+\n",
      "|stn   |year|mo |rt_chg_temp         |\n",
      "+------+----+---+--------------------+\n",
      "|010014|2018|02 |-0.17159955688276657|\n",
      "|010060|2018|09 |-0.4721980771763467 |\n",
      "|010060|2018|11 |-0.21319905213270146|\n",
      "|010070|2018|01 |0.08330645161290319 |\n",
      "|010070|2018|04 |0.35804226918798654 |\n",
      "+------+----+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def rate_of_change_temperature(day: pd.Series, temp: pd.Series) -> float:\n",
    "    \"\"\"Returns the slope of the daily temperature for a given period of time.\"\"\"\n",
    "    return (\n",
    "        LinearRegression()\n",
    "        .fit(X=day.astype(int).values.reshape(-1, 1), y=temp)\n",
    "        .coef_[0]\n",
    "    )\n",
    "\n",
    "result = gsod.groupby(\"stn\", \"year\", \"mo\").agg(\n",
    "    rate_of_change_temperature(gsod[\"da\"], gsod[\"temp\"]).alias(\n",
    "        \"rt_chg_temp\"\n",
    "    )\n",
    ")\n",
    "\n",
    "result.show(5,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group map UDF\n",
    "\n",
    "The second type of UDF on grouped data is the group map UDF. Unlike the group\n",
    "aggregate UDF, which returns a scalar value as a result over a batch, the grouped map UDF maps over each batch and returns a (pandas) data frame that gets combined\n",
    "back into a single (Spark) data frame.\n",
    "Scalar UDFs relied on pandas Series, group map UDFs use pandas DataFrame.\n",
    "Each logical batch from step 1 in figure above becomes a pandas DataFrame ready for action. Our function must return a complete DataFrame, meaning that all the columns we want to display need to be returned, including the one we grouped against.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+------------------+\n",
      "|stn   |year|mo |da |temp|temp_norm         |\n",
      "+------+----+---+---+----+------------------+\n",
      "|010014|2018|02 |25 |35.6|0.859090909090909 |\n",
      "|010014|2018|02 |20 |35.5|0.8545454545454544|\n",
      "|010014|2018|02 |06 |29.8|0.5954545454545455|\n",
      "|010014|2018|02 |15 |35.4|0.8499999999999999|\n",
      "|010014|2018|02 |19 |36.8|0.9136363636363634|\n",
      "+------+----+---+---+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scale_temperature(temp_by_day: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns a simple normalization of the temperature for a site.\n",
    "    If the temperature is constant for the whole window, defaults to 0.5.\"\"\"\n",
    "    temp = temp_by_day.temp\n",
    "    answer = temp_by_day[[\"stn\", \"year\", \"mo\", \"da\", \"temp\"]]\n",
    "    \n",
    "    if temp.min() == temp.max():\n",
    "        return answer.assign(temp_norm=0.5)\n",
    "    \n",
    "    return answer.assign(\n",
    "        temp_norm=(temp - temp.min()) / (temp.max() - temp.min())\n",
    "    )\n",
    "\n",
    "gsod_map = gsod.groupby(\"stn\", \"year\", \"mo\").applyInPandas(\n",
    "    scale_temperature,\n",
    "    schema=(\n",
    "        \"stn string, year string, mo string, \"     # StructType syntax can also be used\n",
    "        \"da string, temp double, temp_norm double\" # similar to 6_PySpark_w_JSON.ipynb\n",
    "    ),\n",
    ")\n",
    "\n",
    "gsod_map.show(5, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group map UDFs are highly flexible constructs: as long as you respect the schema you provide to the `applyInPandas()`, Spark will not require that you keep the same (or any) number of records. This is as close as we will get to treating a Spark data frame like a predetermined collection (via `groupby()`) of a pandas DataFrame. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to use, when\n",
    "\n",
    "- If you need to control how the batches are made, you need to use a grouped data UDF. If the return value is scalar, group aggregate, or otherwise, use a group map and return a transformed (complete) data frame.\n",
    "- If you only want batches, you have more options. The most flexible is mapInPandas(), where an iterator of pandas DataFrame comes in and a transformed one comes out. This is very useful when you want to distribute a pandas/local data transformation on the whole data frame, such as with inference of local ML models. Use it if you work with most of the columns from the data frame, and use a Series to Series UDF if you only need a few columns.\n",
    "- If you have a cold-start process, use a Iterator of Series/multiple Series UDF, depending on the number of columns you need within your UDF.\n",
    "- Finally, if you only need to transform some columns using pandas, a Series to Series UDF is the way to go.\n",
    "\n",
    "<img src=\"images/group_udf_when.png\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important aspect of a pandas UDF (and any UDF) is that it needs to work on the nondistributed version of your data. For regular UDFs, this means passing *any argument of the type of values you expect* should yield an answer. For instance, if you divide an array of values by another one, you need to cover the case of dividing by zero. The same is true for any pandas UDF: you need to be lenient with the input you accept and strict with the output you provide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
