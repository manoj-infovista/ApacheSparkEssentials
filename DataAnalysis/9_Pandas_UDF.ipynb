{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data: Using pandas UDFs\n",
    "\n",
    "PySpark’s interoperability with pandas (also colloquially called pandas UDF) is a huge selling point when performing data analysis at scale. pandas is the dominant in-memory Python data manipulation library, while PySpark is the dominantly distributed one. Combining both of them unlocks additional possibilities.\n",
    "\n",
    "We will look into operations on `GroupedData` and how PySpark plus Pandas implement the split-apply-combine pattern common to data analysis. We finish with the ultimate interaction between pandas and PySpark: treating a PySpark data frame like a small collection of pandas DataFrames."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column transformations with pandas: Using Series UDF\n",
    "\n",
    "The Series UDFs family shares a column-first focus with regular PySpark data transformation functions. All of our UDFs in this section will take a Column object (or objects) as input and return a Column object as output. \n",
    "\n",
    "PySpark provides three types of Series UDFs.\n",
    "- The *Series to Series* takes `Columns` objects as inputs, converts them to pandas Series objects, and returns a Series object that gets promoted back to a PySpark Column object.\n",
    "- The *Iterator of Series to Iterator of Series*  differs in the sense that the `Column` objects get batched into batches and then fed as Iterator objects. It takes a single Column object as input and returns a single `Column`.\n",
    "- The *Iterator of multiple Series to Iterator of Series* is a combination of the previous Series UDFs and can take multiple Columns as input, like the Series to Series UDF, yet preserves the iterator pattern from the Iterator of Series to Iterator of Series.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting Spark to Google's BigQuery\n",
    "\n",
    "We connect PySpark to Google’s BigQuery, where we will use the National Oceanic and Atmospheric Administration’s (NOAA) Global Surface Summary of the Day (GSOD) data set. In the same vein, this provides a blueprint for connecting PySpark to other data warehouses, such as SQL or NoSQL databases. \n",
    "\n",
    "You need a GCP account. Once your account is created, you need to create a service account and a service account key to tell BigQuery to give you access to the public data programmatically. To do so, select Service Account (under IAM & Admin) and click + Create Service Account. Give a meaningful name to your service account.\n",
    "In the service account permissions menu, select BigQuery → BigQuery admin and click\n",
    "Continue. In the last step, click + CREATE KEY and select JSON. Download the key and store it somewhere safe\n",
    "\n",
    "Download the Google's BigQuery connector from [here](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the connection betweek PySpark and BigQuery through connector\n",
    "\n",
    "We instruct Spark to fetch and install external dependencies, in our case, the `com.google.cloud.spark:spark-bigquery connector`. As it is a Java/Scala dependency, we need to match the correct Spark and Scala version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.jars.packages\",\n",
    "    \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.28.0\",\n",
    ").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data from BigQuery using our secret key\n",
    "\n",
    "we can start creating pandas UDFs: we just have to read the data. we assemble 10 years worth of weather data located in BigQuery, which totals over 40 million records.\n",
    "\n",
    "we use the bigqueryspecialized SparkReader—provided by the connector library we embedded to our PySpark shell—which provides two options:\n",
    "- The table parameter pointing to the table we want to ingest. The format is `project.dataset.table`; the `bigquery-public-data` is a project available to all.\n",
    "- The `credentialsFile` is the JSON key downloaded beffore. You need\n",
    "to adjust the path and file name according to the location of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def read_df_from_bq(year):\n",
    "    return (\n",
    "        spark.read.format(\"bigquery\").option(\n",
    "            \"table\", f\"bigquery-public-data.noaa_gsod.gsod{year}\"\n",
    "        )\n",
    "        .option(\"credentialsFile\", \"big-query-spark-key.json\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "\n",
    "gsod = (\n",
    "    reduce(\n",
    "        lambda x, y: x.unionByName(y, allowMissingColumns=True),\n",
    "        [read_df_from_bq(year) for year in range(2018, 2019)],\n",
    "    )\n",
    "    .dropna(subset=[\"year\", \"mo\", \"da\", \"temp\"])\n",
    "    .where(F.col(\"temp\") != 9999.9)\n",
    "    .drop(\"date\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series to Series UDF: Column functions, but with pandas\n",
    "\n",
    "The Series to Series UDF, also called Scalar UDF, are akin to most of the functions in the `pyspark.sql model`. For the most part, they work just like Python UDFs as seen in previous notebook, with one key difference: Python UDFs work on one record at a time, and you express your logic through regular Python code. Scalar UDFs work on one Series at a time, and you express your logic through pandas code. \n",
    "\n",
    "<img src=\"images/series_2_series_udf.png\" width=\"600px\">\n",
    "\n",
    "In a Python UDF, when you pass column objects to your UDF, PySpark will unpack\n",
    "each value, perform the computation, and then return the value for each record in\n",
    "a Column object. Whereas in a Scaler UDF, PySpark will serialize (through a library called PyArrow) each partitioned column into a pandas Series object. You then perform the operations on the Series object directly, returning a Series of\n",
    "the same dimension from your UDF.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple function that will transform Fahrenheit degrees to Celsius. \n",
    "- Instead of `udf()`, we use `pandas_udf()`, again, from the `pyspark.sql.functions` module. Optionally (but recommended), we can pass the return type of the UDF as an argument to the `pandas_udf()` decorator.\n",
    "- Our function signature is also different: rather than using scalar values (such as int or str), the UDF takes `pd.Series` and return a `pd.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def f_to_c(degrees: pd.Series) -> pd.Series:\n",
    "    \"\"\"Transforms Farhenheit to Celcius.\"\"\"\n",
    "    return (degrees - 32) * 5 / 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we apply our newly created Series to Series UDF to the temp column of the gsod data frame, which contains the temperature (in Fahrenheit) of each stationday combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|temp|             temp_c|\n",
      "+----+-------------------+\n",
      "|29.6|-1.3333333333333326|\n",
      "|53.5| 11.944444444444445|\n",
      "|71.6| 21.999999999999996|\n",
      "|70.4| 21.333333333333336|\n",
      "|37.2| 2.8888888888888906|\n",
      "+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod = gsod.withColumn(\"temp_c\", f_to_c(F.col(\"temp\")))\n",
    "gsod.select(\"temp\", \"temp_c\").distinct().show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar UDF + cold start = Iterator of Series UDF\n",
    "\n",
    "This section combines the other two types of Scalar UDFs: the *Iterator of Series to Iterator of Series* UDF and the *Iterator of multiple Series to Iterator of Series*. \n",
    "\n",
    "Iterator of Series UDFs are very useful when you have an expensive cold start operation you need to perform. By cold start, we mean an operation we need to perform once at the beginning of the processing step, before working through the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|temp|             temp_c|\n",
      "+----+-------------------+\n",
      "|29.6|-1.3333333333333326|\n",
      "|53.5| 11.944444444444445|\n",
      "|71.6| 21.999999999999996|\n",
      "|70.4| 21.333333333333336|\n",
      "|37.2| 2.8888888888888906|\n",
      "+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from typing import Iterator\n",
    "\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def f_to_c2(degrees: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Transforms Farhenheit to Celcius.\"\"\"\n",
    "    # We simulate a cold start using sleep() for five seconds. \n",
    "    # The cold start will happen on each worker\n",
    "    # once, rather than for every batch\n",
    "    sleep(5)\n",
    "     \n",
    "    for batch in degrees:\n",
    "        yield (batch - 32) * 5 / 9\n",
    "\n",
    "\n",
    "gsod.select(\n",
    "    \"temp\", f_to_c2(F.col(\"temp\")).alias(\"temp_c\")\n",
    ").distinct().show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Iterator of multiple Series to Iterator of Series is a special case to wrap multiple columns in a single iterator. We'll assemble the year, mo, and da columns\n",
    "(representing the year, month, and day) into a single column. This example requires more data transformation than when using an Iterator of a single Series.\n",
    "\n",
    "Our date assembly UDF works like this:\n",
    "1. year_mo_da is an Iterator of a tuple of Series, representing all the batches of values contained in the year, mo, and da columns.\n",
    "2. To access each batch, we use a for loop over the iterator, the same principle as for the Iterator of Series UDF.\n",
    "3. To extract each individual series from the tuple, we use multiple assignments.\n",
    "In this case, year will map to the first Series of the tuple, mo to the second, and da to the third.\n",
    "4. Since pd.to_datetime requests a data frame containing the year, month, and\n",
    "day columns, we create the data frame via a dictionary, giving the keys the relevant column names. pd.to_datetime returns a Series.\n",
    "5. Finally, we yield the answer to build the Iterator of Series, fulfilling our contract.\n",
    "\n",
    "<img src=\"images/iterator_of_mutl_series.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+----------+\n",
      "|year| mo| da|      date|\n",
      "+----+---+---+----------+\n",
      "|2018| 08| 21|2018-08-21|\n",
      "|2018| 07| 29|2018-07-29|\n",
      "|2018| 05| 12|2018-05-12|\n",
      "|2018| 03| 20|2018-03-20|\n",
      "|2018| 09| 11|2018-09-11|\n",
      "+----+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "@F.pandas_udf(T.DateType())\n",
    "def create_date(year_mo_da: Iterator[Tuple[pd.Series, pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Merges three cols (representing Y-M-D of a date) into a Date col.\"\"\"\n",
    "    for year, mo, da in year_mo_da:\n",
    "        yield pd.to_datetime(\n",
    "            pd.DataFrame(dict(year=year, month=mo, day=da))\n",
    "        )\n",
    "\n",
    "\n",
    "gsod.select(\n",
    "    \"year\", \"mo\", \"da\",\n",
    "    create_date(F.col(\"year\"), F.col(\"mo\"), F.col(\"da\")).alias(\"date\"),\n",
    ").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
