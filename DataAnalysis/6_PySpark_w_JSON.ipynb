{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional data frames: Using PySpark with JSON data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have used PySpark’s data frame to work with textual and tabular data.\n",
    "Let's push the abstraction a little further by representing *hierarchical information* within a data frame. Imagine it for a moment: columns within columns, the ultimate flexibility."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading JSON data\n",
    "\n",
    "This section explains what JSON is, how to use the specialized JSON reader with\n",
    "PySpark, and how a JSON file is represented within a data frame. For this chapter, we use a JSON dump of information about the TV show Silicon Valley from TV Maze.\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"id\":143,\n",
    "   \"name\":\"Silicon Valley\",\n",
    "   \"type\":\"Scripted\",\n",
    "   \"language\":\"English\",\n",
    "   \"genres\":[\n",
    "      \"Comedy\"\n",
    "   ],\n",
    "   \"network\":{\n",
    "      \"id\":8,\n",
    "      \"name\":\"HBO\",\n",
    "      \"country\":{\n",
    "         \"name\":\"United States\",\n",
    "         \"code\":\"US\",\n",
    "         \"timezone\":\"America/New_York\"\n",
    "      }\n",
    "   },\n",
    "   \"_embedded\":{\n",
    "      \"episodes\":[\n",
    "         {\n",
    "            \"id\":10897,\n",
    "            \"name\":\"Minimum Viable Product\",\n",
    "            \"season\":1,\n",
    "            \"number\":1\n",
    "         },\n",
    "         {\n",
    "            \"id\":10898,\n",
    "            \"name\":\"The Cap Table\",\n",
    "            \"season\":1,\n",
    "            \"number\":2\n",
    "         }\n",
    "      ]\n",
    "   }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 143, 'name': 'Silicon Valley', 'type': 'Scripted', 'language': 'English', 'genres': ['Comedy'], 'network': {'id': 8, 'name': 'HBO', 'country': {'name': 'United States', 'code': 'US', 'timezone': 'America/New_York'}}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Reading a simple JSON document as python dictionary\n",
    "import json\n",
    "sample_json = \"\"\"{\n",
    "   \"id\":143,\n",
    "   \"name\":\"Silicon Valley\",\n",
    "   \"type\":\"Scripted\",\n",
    "   \"language\":\"English\",\n",
    "   \"genres\":[\n",
    "      \"Comedy\"\n",
    "   ],\n",
    "   \"network\":{\n",
    "      \"id\":8,\n",
    "      \"name\":\"HBO\",\n",
    "      \"country\":{\n",
    "         \"name\":\"United States\",\n",
    "         \"code\":\"US\",\n",
    "         \"timezone\":\"America/New_York\"\n",
    "      }\n",
    "   }\n",
    "}\"\"\"\n",
    "document = json.loads(sample_json)\n",
    "print(document)\n",
    "print(type(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "DIRECTORY = \"data/shows/\"\n",
    "\n",
    "shows = spark.read.json(\n",
    "    os.path.join(DIRECTORY,'shows-silicon-valley.json')\n",
    ")\n",
    "\n",
    "shows.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the PySpark world, reading JSON follows this rule: *one JSON document, one line, one record*. This means that if you want to have multiple JSON records in the same document, you need to have one document per line and no new line within your document.\n",
    "\n",
    "If you want to ingest multiple documents across multiple files, you need to set\n",
    "the `multiLine` (careful about the capital L!) parameter to `true`. This will changethe JSON reading rule to the following: *one JSON document, one file, one record*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_shows = spark.read.json(\n",
    "    os.path.join(DIRECTORY,\"shows-*.json\"),\n",
    "    multiLine=True)\n",
    "    \n",
    "three_shows.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex data types\n",
    "ARRAY, MAP, STRUCT\n",
    "\n",
    "Spark uses this the term *complex data types* to refer to data types that contain other types.  In PySpark, we have the *array*, the *map*, and the *struct*. With these, you will be able to express an infinite amount of data layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _embedded: struct (nullable = true)\n",
      " |    |-- episodes: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |    |-- airstamp: string (nullable = true)\n",
      " |    |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- number: long (nullable = true)\n",
      " |    |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |    |-- season: long (nullable = true)\n",
      " |    |    |    |-- summary: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |-- _links: struct (nullable = true)\n",
      " |    |-- previousepisode: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |    |-- self: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |-- externals: struct (nullable = true)\n",
      " |    |-- imdb: string (nullable = true)\n",
      " |    |-- thetvdb: long (nullable = true)\n",
      " |    |-- tvrage: long (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- medium: string (nullable = true)\n",
      " |    |-- original: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- network: struct (nullable = true)\n",
      " |    |-- country: struct (nullable = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- timezone: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- officialSite: string (nullable = true)\n",
      " |-- premiered: string (nullable = true)\n",
      " |-- rating: struct (nullable = true)\n",
      " |    |-- average: double (nullable = true)\n",
      " |-- runtime: long (nullable = true)\n",
      " |-- schedule: struct (nullable = true)\n",
      " |    |-- days: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- updated: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- webChannel: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_embedded', '_links', 'externals', 'genres', 'id', 'image', 'language', 'name', 'network', 'officialSite', 'premiered', 'rating', 'runtime', 'schedule', 'status', 'summary', 'type', 'updated', 'url', 'webChannel', 'weight']\n"
     ]
    }
   ],
   "source": [
    "# the hierarchy within the schema. \n",
    "# PySpark took every top-level key—the keys \n",
    "# from the root object—and parsed them as columns \n",
    "\n",
    "print(shows.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|name          |genres  |\n",
      "+--------------+--------+\n",
      "|Silicon Valley|[Comedy]|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset = shows.select(\"name\",\"genres\")\n",
    "\n",
    "array_subset.show(1,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our genres column can be thought of as containing lists of elements within each record. To get to the value inside the array, we need to extract them. PySpark provides a very pythonic way to work with arrays as if they were lists.\n",
    "\n",
    "PySpark’s array functions—available in the pyspark.sql.functions module—are\n",
    "almost all prefixed with the `array_` keyword\n",
    "\n",
    "Let's play with these functions.\n",
    "\n",
    "- We create three literal columns (using `lit()` to create scalar columns, then\n",
    "`make_array()`) to create an array of possible genres. PySpark won’t accept\n",
    "Python lists as an argument to `lit()`, so we have to go the long route by creating individual scalar columns before combining them into a single array.\n",
    "- Then use the function `array_repeat()` to create a column repeating the Comedy string we extracted below five times. Finally compute the size of both\n",
    "columns, de-dupe both arrays, and intersect them, yielding our original [Comedy] array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------------+--------------+--------------+\n",
      "|          name|dot_and_index|col_and_index|dot_and_method|col_and_method|\n",
      "+--------------+-------------+-------------+--------------+--------------+\n",
      "|Silicon Valley|       Comedy|       Comedy|        Comedy|        Comedy|\n",
      "+--------------+-------------+-------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset = array_subset.select(\n",
    "    \"name\",\n",
    "    array_subset.genres[0].alias(\"dot_and_index\"),\n",
    "    F.col(\"genres\")[0].alias(\"col_and_index\"),\n",
    "    array_subset.genres.getItem(0).alias(\"dot_and_method\"),\n",
    "    F.col(\"genres\").getItem(0).alias(\"col_and_method\"),\n",
    ")\n",
    "array_subset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+----------------------------------------+\n",
      "|name          |Some_Genres            |Repeated_Genres                         |\n",
      "+--------------+-----------------------+----------------------------------------+\n",
      "|Silicon Valley|[Comedy, Horror, Drama]|[Comedy, Comedy, Comedy, Comedy, Comedy]|\n",
      "+--------------+-----------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated = array_subset.select(\n",
    "    \"name\",\n",
    "    F.lit(\"Comedy\").alias(\"one\"),\n",
    "    F.lit(\"Horror\").alias(\"two\"),\n",
    "    F.lit(\"Drama\").alias(\"three\"),\n",
    "    F.col(\"dot_and_index\"),\n",
    ").select(\n",
    "    \"name\",\n",
    "    F.array(\"one\", \"two\", \"three\").alias(\"Some_Genres\"),\n",
    "    F.array_repeat(\"dot_and_index\", 5).alias(\"Repeated_Genres\"),\n",
    ")\n",
    "\n",
    "array_subset_repeated.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+---------------------+\n",
      "|          name|size(Some_Genres)|size(Repeated_Genres)|\n",
      "+--------------+-----------------+---------------------+\n",
      "|Silicon Valley|                3|                    5|\n",
      "+--------------+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated.select(\n",
    "    \"name\", F.size(\"Some_Genres\"), F.size(\"Repeated_Genres\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------------+-------------------------------+\n",
      "|name          |array_distinct(Some_Genres)|array_distinct(Repeated_Genres)|\n",
      "+--------------+---------------------------+-------------------------------+\n",
      "|Silicon Valley|[Comedy, Horror, Drama]    |[Comedy]                       |\n",
      "+--------------+---------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated.select(\n",
    "    \"name\",\n",
    "    F.array_distinct(\"Some_Genres\"),\n",
    "    F.array_distinct(\"Repeated_Genres\"),\n",
    ").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|          name|  Genres|\n",
      "+--------------+--------+\n",
      "|Silicon Valley|[Comedy]|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated = array_subset_repeated.select(\n",
    "    \"name\",\n",
    "    F.array_intersect(\"Some_Genres\", \"Repeated_Genres\").alias(\"Genres\"),\n",
    ")\n",
    "array_subset_repeated.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to know the position of a value in an array, you can use `array_position()`. This function takes two arguments:\n",
    "- An array column to perform the search\n",
    "- A value to search for within the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------+\n",
      "|  Genres|array_position(Genres, Comedy)|\n",
      "+--------+------------------------------+\n",
      "|[Comedy]|                             1|\n",
      "+--------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated.select(\n",
    "    \"Genres\", F.array_position(\"Genres\", \"Comedy\")\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP type: keys and values within a column\n",
    "\n",
    "A map is conceptually very close to a Python typed dictionary: you have keys and values just like in a dictionary, but as with the array, the keys need to be of the same type, and the values need to be of the same type (the type for the keys can be different than the type for the values). Values can be null, but keys can’t, just like with Python dictionaries.\n",
    "\n",
    "One of the easiest ways to create a map is from two columns of type array. We will\n",
    "do so by collecting some information about the name, language, type, and url columns into an array and using the map_from_arrays() function, like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------------------------+\n",
      "|keys                  |values                             |\n",
      "+----------------------+-----------------------------------+\n",
      "|[name, language, type]|[Silicon Valley, English, Scripted]|\n",
      "+----------------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"name\", \"language\", \"type\"]\n",
    "\n",
    "shows_map = shows.select(\n",
    "    *[F.lit(column) for column in columns],\n",
    "    F.array(*columns).alias(\"values\"),\n",
    ")\n",
    "shows_map = shows_map.select(F.array(*columns).alias(\"keys\"), \"values\")\n",
    "shows_map.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mapped: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows_map = shows_map.select(\n",
    "    F.map_from_arrays(\"keys\", \"values\").alias(\"mapped\")\n",
    ")\n",
    "\n",
    "shows_map.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|mapped                                                         |\n",
      "+---------------------------------------------------------------+\n",
      "|{name -> Silicon Valley, language -> English, type -> Scripted}|\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows_map.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------+\n",
      "|          name|  mapped[name]|  mapped[name]|\n",
      "+--------------+--------------+--------------+\n",
      "|Silicon Valley|Silicon Valley|Silicon Valley|\n",
      "+--------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To access the mapped values\n",
    "shows_map.select(\n",
    "    F.col(\"mapped.name\"),\n",
    "    F.col(\"mapped\")[\"name\"],\n",
    "    shows_map.mapped[\"name\"],\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with the `array`, PySpark provides a few functions to work with maps under the `pyspark.sql.functions` module. Most of them are prefixed or suffixed with `map`, such as `map_values()` (which creates an array column out of the map values) or `create_map()` (which creates a map from the columns passed as a parameter, alternating between keys and values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct: Nesting columns within columns\n",
    "\n",
    "The struct is akin to a JSON object, in the sense that the key or name of each pair is a string and that each record can be of a different type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- schedule: struct (nullable = true)\n",
      " |    |-- days: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows.select(\"schedule\").printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The struct is very different from the `array` and the `map` in that the number of fields and their names are known ahead of time. In our case, the `schedule` struct column is fixed: we know that each record of our data frame will contain that `schedule` struct (or a `null` value, if we want to be pedantic), and within that struct there will be an array of strings, `days`, and a string, `time`. The `array` and the `map` enforce the types of the values, but not their numbers or names. The struct allows for more versatility of types, as long as you name each field and provide the type ahead of time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can visualize that schedule is a data frame of two columns (days and time)\n",
    "trapped within the column.\n",
    "\n",
    "![struct data type](images/with_json_struct.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigating structs as if they were nested columns\n",
    "how to extract values from nested structs inside a data frame. PySpark provides the same convenience when working with nested columns as it would for regular columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _embedded: struct (nullable = true)\n",
      " |    |-- episodes: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |    |-- airstamp: string (nullable = true)\n",
      " |    |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- number: long (nullable = true)\n",
      " |    |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |    |-- season: long (nullable = true)\n",
      " |    |    |    |-- summary: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows.select(F.col(\"_embedded\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _links: struct (nullable = true)\n",
      " |    |-- previousepisode: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |    |-- self: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |-- externals: struct (nullable = true)\n",
      " |    |-- imdb: string (nullable = true)\n",
      " |    |-- thetvdb: long (nullable = true)\n",
      " |    |-- tvrage: long (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- medium: string (nullable = true)\n",
      " |    |-- original: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- network: struct (nullable = true)\n",
      " |    |-- country: struct (nullable = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- timezone: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- officialSite: string (nullable = true)\n",
      " |-- premiered: string (nullable = true)\n",
      " |-- rating: struct (nullable = true)\n",
      " |    |-- average: double (nullable = true)\n",
      " |-- runtime: long (nullable = true)\n",
      " |-- schedule: struct (nullable = true)\n",
      " |    |-- days: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- updated: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- webChannel: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      " |-- episodes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |-- airstamp: string (nullable = true)\n",
      " |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- number: long (nullable = true)\n",
      " |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |-- season: long (nullable = true)\n",
      " |    |    |-- summary: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows_clean = shows.withColumn(\n",
    "    \"episodes\", F.col(\"_embedded.episodes\")\n",
    ").drop(\"_embedded\")\n",
    "\n",
    "shows_clean.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can refer to individual elements in the array using the index in brackets after\n",
    "the column reference. What about extracting the names of all the episodes, which are within the episodes array of structs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes_name = shows_clean.select(F.col(\"episodes.name\"))\n",
    "episodes_name.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|name                     |\n",
      "+-------------------------+\n",
      "|Minimum Viable Product   |\n",
      "|The Cap Table            |\n",
      "|Articles of Incorporation|\n",
      "+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes_name.select(F.explode(\"name\").alias(\"name\")).show(3, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and using the data frame schema\n",
    "Let's see how to define and use a schema with a PySpark data frame.We build the schema for our JSON object programmatically and review the out-ofthe-box types  PySpark offers. Being able to use Python structures (serialized as JSON) means that we can manipulate our schemas just like any other data structure; we can reuse our data manipulation tool kit for manipulating our data frame’s metadata. By doing this, we also address the potential slowdown from inferSchema, as we\n",
    "don’t need Spark to read the data twice (once to infer the schema, once to perform\n",
    "the read).\n",
    "\n",
    "#### Using Spark types as the base blocks of a schema\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
