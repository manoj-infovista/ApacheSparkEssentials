{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional data frames: Using PySpark with JSON data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have used PySpark’s data frame to work with textual and tabular data.\n",
    "Let's push the abstraction a little further by representing *hierarchical information* within a data frame. Imagine it for a moment: columns within columns, the ultimate flexibility."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading JSON data\n",
    "\n",
    "This section explains what JSON is, how to use the specialized JSON reader with\n",
    "PySpark, and how a JSON file is represented within a data frame. For this chapter, we use a JSON dump of information about the TV show Silicon Valley from TV Maze.\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"id\":143,\n",
    "   \"name\":\"Silicon Valley\",\n",
    "   \"type\":\"Scripted\",\n",
    "   \"language\":\"English\",\n",
    "   \"genres\":[\n",
    "      \"Comedy\"\n",
    "   ],\n",
    "   \"network\":{\n",
    "      \"id\":8,\n",
    "      \"name\":\"HBO\",\n",
    "      \"country\":{\n",
    "         \"name\":\"United States\",\n",
    "         \"code\":\"US\",\n",
    "         \"timezone\":\"America/New_York\"\n",
    "      }\n",
    "   },\n",
    "   \"_embedded\":{\n",
    "      \"episodes\":[\n",
    "         {\n",
    "            \"id\":10897,\n",
    "            \"name\":\"Minimum Viable Product\",\n",
    "            \"season\":1,\n",
    "            \"number\":1\n",
    "         },\n",
    "         {\n",
    "            \"id\":10898,\n",
    "            \"name\":\"The Cap Table\",\n",
    "            \"season\":1,\n",
    "            \"number\":2\n",
    "         }\n",
    "      ]\n",
    "   }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 143, 'name': 'Silicon Valley', 'type': 'Scripted', 'language': 'English', 'genres': ['Comedy'], 'network': {'id': 8, 'name': 'HBO', 'country': {'name': 'United States', 'code': 'US', 'timezone': 'America/New_York'}}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Reading a simple JSON document as python dictionary\n",
    "import json\n",
    "sample_json = \"\"\"{\n",
    "   \"id\":143,\n",
    "   \"name\":\"Silicon Valley\",\n",
    "   \"type\":\"Scripted\",\n",
    "   \"language\":\"English\",\n",
    "   \"genres\":[\n",
    "      \"Comedy\"\n",
    "   ],\n",
    "   \"network\":{\n",
    "      \"id\":8,\n",
    "      \"name\":\"HBO\",\n",
    "      \"country\":{\n",
    "         \"name\":\"United States\",\n",
    "         \"code\":\"US\",\n",
    "         \"timezone\":\"America/New_York\"\n",
    "      }\n",
    "   }\n",
    "}\"\"\"\n",
    "document = json.loads(sample_json)\n",
    "print(document)\n",
    "print(type(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "DIRECTORY = \"data/shows/\"\n",
    "\n",
    "shows = spark.read.json(\n",
    "    os.path.join(DIRECTORY,'shows-silicon-valley.json')\n",
    ")\n",
    "\n",
    "shows.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the PySpark world, reading JSON follows this rule: *one JSON document, one line, one record*. This means that if you want to have multiple JSON records in the same document, you need to have one document per line and no new line within your document.\n",
    "\n",
    "If you want to ingest multiple documents across multiple files, you need to set\n",
    "the `multiLine` (careful about the capital L!) parameter to `true`. This will changethe JSON reading rule to the following: *one JSON document, one file, one record*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_shows = spark.read.json(\n",
    "    os.path.join(DIRECTORY,\"shows-*.json\"),\n",
    "    multiLine=True)\n",
    "    \n",
    "three_shows.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex data types\n",
    "ARRAY, MAP, STRUCT\n",
    "\n",
    "Spark uses this the term *complex data types* to refer to data types that contain other types.  In PySpark, we have the *array*, the *map*, and the *struct*. With these, you will be able to express an infinite amount of data layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _embedded: struct (nullable = true)\n",
      " |    |-- episodes: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |    |-- airstamp: string (nullable = true)\n",
      " |    |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- number: long (nullable = true)\n",
      " |    |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |    |-- season: long (nullable = true)\n",
      " |    |    |    |-- summary: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |-- _links: struct (nullable = true)\n",
      " |    |-- previousepisode: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |    |-- self: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |-- externals: struct (nullable = true)\n",
      " |    |-- imdb: string (nullable = true)\n",
      " |    |-- thetvdb: long (nullable = true)\n",
      " |    |-- tvrage: long (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- medium: string (nullable = true)\n",
      " |    |-- original: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- network: struct (nullable = true)\n",
      " |    |-- country: struct (nullable = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- timezone: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- officialSite: string (nullable = true)\n",
      " |-- premiered: string (nullable = true)\n",
      " |-- rating: struct (nullable = true)\n",
      " |    |-- average: double (nullable = true)\n",
      " |-- runtime: long (nullable = true)\n",
      " |-- schedule: struct (nullable = true)\n",
      " |    |-- days: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- updated: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- webChannel: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_embedded', '_links', 'externals', 'genres', 'id', 'image', 'language', 'name', 'network', 'officialSite', 'premiered', 'rating', 'runtime', 'schedule', 'status', 'summary', 'type', 'updated', 'url', 'webChannel', 'weight']\n"
     ]
    }
   ],
   "source": [
    "# the hierarchy within the schema. \n",
    "# PySpark took every top-level key—the keys \n",
    "# from the root object—and parsed them as columns \n",
    "\n",
    "print(shows.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|name          |genres  |\n",
      "+--------------+--------+\n",
      "|Silicon Valley|[Comedy]|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset = shows.select(\"name\",\"genres\")\n",
    "\n",
    "array_subset.show(1,False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our genres column can be thought of as containing lists of elements within each record. To get to the value inside the array, we need to extract them. PySpark provides a very pythonic way to work with arrays as if they were lists.\n",
    "\n",
    "PySpark’s array functions—available in the pyspark.sql.functions module—are\n",
    "almost all prefixed with the `array_` keyword\n",
    "\n",
    "Let's play with these functions.\n",
    "\n",
    "- We create three literal columns (using `lit()` to create scalar columns, then\n",
    "`make_array()`) to create an array of possible genres. PySpark won’t accept\n",
    "Python lists as an argument to `lit()`, so we have to go the long route by creating individual scalar columns before combining them into a single array.\n",
    "- Then use the function `array_repeat()` to create a column repeating the Comedy string we extracted below five times. Finally compute the size of both\n",
    "columns, de-dupe both arrays, and intersect them, yielding our original [Comedy] array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------------+--------------+--------------+\n",
      "|          name|dot_and_index|col_and_index|dot_and_method|col_and_method|\n",
      "+--------------+-------------+-------------+--------------+--------------+\n",
      "|Silicon Valley|       Comedy|       Comedy|        Comedy|        Comedy|\n",
      "+--------------+-------------+-------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset = array_subset.select(\n",
    "    \"name\",\n",
    "    array_subset.genres[0].alias(\"dot_and_index\"),\n",
    "    F.col(\"genres\")[0].alias(\"col_and_index\"),\n",
    "    array_subset.genres.getItem(0).alias(\"dot_and_method\"),\n",
    "    F.col(\"genres\").getItem(0).alias(\"col_and_method\"),\n",
    ")\n",
    "array_subset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+----------------------------------------+\n",
      "|name          |Some_Genres            |Repeated_Genres                         |\n",
      "+--------------+-----------------------+----------------------------------------+\n",
      "|Silicon Valley|[Comedy, Horror, Drama]|[Comedy, Comedy, Comedy, Comedy, Comedy]|\n",
      "+--------------+-----------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated = array_subset.select(\n",
    "    \"name\",\n",
    "    F.lit(\"Comedy\").alias(\"one\"),\n",
    "    F.lit(\"Horror\").alias(\"two\"),\n",
    "    F.lit(\"Drama\").alias(\"three\"),\n",
    "    F.col(\"dot_and_index\"),\n",
    ").select(\n",
    "    \"name\",\n",
    "    F.array(\"one\", \"two\", \"three\").alias(\"Some_Genres\"),\n",
    "    F.array_repeat(\"dot_and_index\", 5).alias(\"Repeated_Genres\"),\n",
    ")\n",
    "\n",
    "array_subset_repeated.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+---------------------+\n",
      "|          name|size(Some_Genres)|size(Repeated_Genres)|\n",
      "+--------------+-----------------+---------------------+\n",
      "|Silicon Valley|                3|                    5|\n",
      "+--------------+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated.select(\n",
    "    \"name\", F.size(\"Some_Genres\"), F.size(\"Repeated_Genres\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------------+-------------------------------+\n",
      "|name          |array_distinct(Some_Genres)|array_distinct(Repeated_Genres)|\n",
      "+--------------+---------------------------+-------------------------------+\n",
      "|Silicon Valley|[Comedy, Horror, Drama]    |[Comedy]                       |\n",
      "+--------------+---------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated.select(\n",
    "    \"name\",\n",
    "    F.array_distinct(\"Some_Genres\"),\n",
    "    F.array_distinct(\"Repeated_Genres\"),\n",
    ").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|          name|  Genres|\n",
      "+--------------+--------+\n",
      "|Silicon Valley|[Comedy]|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated = array_subset_repeated.select(\n",
    "    \"name\",\n",
    "    F.array_intersect(\"Some_Genres\", \"Repeated_Genres\").alias(\"Genres\"),\n",
    ")\n",
    "array_subset_repeated.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to know the position of a value in an array, you can use `array_position()`. This function takes two arguments:\n",
    "- An array column to perform the search\n",
    "- A value to search for within the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------+\n",
      "|  Genres|array_position(Genres, Comedy)|\n",
      "+--------+------------------------------+\n",
      "|[Comedy]|                             1|\n",
      "+--------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated.select(\n",
    "    \"Genres\", F.array_position(\"Genres\", \"Comedy\")\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP type: keys and values within a column\n",
    "\n",
    "A map is conceptually very close to a Python typed dictionary: you have keys and values just like in a dictionary, but as with the array, the keys need to be of the same type, and the values need to be of the same type (the type for the keys can be different than the type for the values). Values can be null, but keys can’t, just like with Python dictionaries.\n",
    "\n",
    "One of the easiest ways to create a map is from two columns of type array. We will\n",
    "do so by collecting some information about the name, language, type, and url columns into an array and using the map_from_arrays() function, like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------------------------+\n",
      "|keys                  |values                             |\n",
      "+----------------------+-----------------------------------+\n",
      "|[name, language, type]|[Silicon Valley, English, Scripted]|\n",
      "+----------------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"name\", \"language\", \"type\"]\n",
    "\n",
    "shows_map = shows.select(\n",
    "    *[F.lit(column) for column in columns],\n",
    "    F.array(*columns).alias(\"values\"),\n",
    ")\n",
    "shows_map = shows_map.select(F.array(*columns).alias(\"keys\"), \"values\")\n",
    "shows_map.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mapped: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows_map = shows_map.select(\n",
    "    F.map_from_arrays(\"keys\", \"values\").alias(\"mapped\")\n",
    ")\n",
    "\n",
    "shows_map.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|mapped                                                         |\n",
      "+---------------------------------------------------------------+\n",
      "|{name -> Silicon Valley, language -> English, type -> Scripted}|\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows_map.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------+\n",
      "|          name|  mapped[name]|  mapped[name]|\n",
      "+--------------+--------------+--------------+\n",
      "|Silicon Valley|Silicon Valley|Silicon Valley|\n",
      "+--------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To access the mapped values\n",
    "shows_map.select(\n",
    "    F.col(\"mapped.name\"),\n",
    "    F.col(\"mapped\")[\"name\"],\n",
    "    shows_map.mapped[\"name\"],\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with the `array`, PySpark provides a few functions to work with maps under the `pyspark.sql.functions` module. Most of them are prefixed or suffixed with `map`, such as `map_values()` (which creates an array column out of the map values) or `create_map()` (which creates a map from the columns passed as a parameter, alternating between keys and values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct: Nesting columns within columns\n",
    "\n",
    "The struct is akin to a JSON object, in the sense that the key or name of each pair is a string and that each record can be of a different type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- schedule: struct (nullable = true)\n",
      " |    |-- days: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows.select(\"schedule\").printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The struct is very different from the `array` and the `map` in that the number of fields and their names are known ahead of time. In our case, the `schedule` struct column is fixed: we know that each record of our data frame will contain that `schedule` struct (or a `null` value, if we want to be pedantic), and within that struct there will be an array of strings, `days`, and a string, `time`. The `array` and the `map` enforce the types of the values, but not their numbers or names. The struct allows for more versatility of types, as long as you name each field and provide the type ahead of time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can visualize that schedule is a data frame of two columns (days and time)\n",
    "trapped within the column.\n",
    "\n",
    "![struct data type](images/with_json_struct.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigating structs as if they were nested columns\n",
    "how to extract values from nested structs inside a data frame. PySpark provides the same convenience when working with nested columns as it would for regular columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _embedded: struct (nullable = true)\n",
      " |    |-- episodes: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |    |-- airstamp: string (nullable = true)\n",
      " |    |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- number: long (nullable = true)\n",
      " |    |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |    |-- season: long (nullable = true)\n",
      " |    |    |    |-- summary: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows.select(F.col(\"_embedded\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _links: struct (nullable = true)\n",
      " |    |-- previousepisode: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |    |-- self: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |-- externals: struct (nullable = true)\n",
      " |    |-- imdb: string (nullable = true)\n",
      " |    |-- thetvdb: long (nullable = true)\n",
      " |    |-- tvrage: long (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- medium: string (nullable = true)\n",
      " |    |-- original: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- network: struct (nullable = true)\n",
      " |    |-- country: struct (nullable = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- timezone: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- officialSite: string (nullable = true)\n",
      " |-- premiered: string (nullable = true)\n",
      " |-- rating: struct (nullable = true)\n",
      " |    |-- average: double (nullable = true)\n",
      " |-- runtime: long (nullable = true)\n",
      " |-- schedule: struct (nullable = true)\n",
      " |    |-- days: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- updated: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- webChannel: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      " |-- episodes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |-- airstamp: string (nullable = true)\n",
      " |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- number: long (nullable = true)\n",
      " |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |-- season: long (nullable = true)\n",
      " |    |    |-- summary: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows_clean = shows.withColumn(\n",
    "    \"episodes\", F.col(\"_embedded.episodes\")\n",
    ").drop(\"_embedded\")\n",
    "\n",
    "shows_clean.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can refer to individual elements in the array using the index in brackets after\n",
    "the column reference. What about extracting the names of all the episodes, which are within the episodes array of structs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes_name = shows_clean.select(F.col(\"episodes.name\"))\n",
    "episodes_name.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|name                     |\n",
      "+-------------------------+\n",
      "|Minimum Viable Product   |\n",
      "|The Cap Table            |\n",
      "|Articles of Incorporation|\n",
      "+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes_name.select(F.explode(\"name\").alias(\"name\")).show(3, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and using the data frame schema\n",
    "Let's see how to define and use a schema with a PySpark data frame.We build the schema for our JSON object programmatically and review the out-ofthe-box types  PySpark offers. Being able to use Python structures (serialized as JSON) means that we can manipulate our schemas just like any other data structure; we can reuse our data manipulation tool kit for manipulating our data frame’s metadata. By doing this, we also address the potential slowdown from inferSchema, as we\n",
    "don’t need Spark to read the data twice (once to infer the schema, once to perform\n",
    "the read).\n",
    "\n",
    "#### Using Spark types as the base blocks of a schema\n",
    "Now we will build the schema for our shows data frame from scratch and include some programmatic niceties of the PySpark schema-building capabilities. The data types we use to build a schema are located in the `pyspark.sql.types` module. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "episode_links_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\n",
    "            \"self\", T.StructType([T.StructField(\"href\", T.StringType())]) # The _links field contains a self struct that itself contains a single-string field: href.\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "episode_image_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"medium\", T.StringType()),\n",
    "        T.StructField(\"original\", T.StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "episode_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"_links\", episode_links_schema),  # <- since types are python\n",
    "        T.StructField(\"airdate\", T.DateType()),         # objects, we can pass them\n",
    "        T.StructField(\"airstamp\", T.TimestampType()),   # to varibles and use them.\n",
    "        T.StructField(\"airtime\", T.StringType()),       # Using episodes_links_schema\n",
    "        T.StructField(\"id\", T.StringType()),            # and episode_image_schema\n",
    "        T.StructField(\"image\", episode_image_schema),   # <- makes our schema for an\n",
    "        T.StructField(\"name\", T.StringType()),          # episode look much cleaner\n",
    "        T.StructField(\"number\", T.LongType()),\n",
    "        T.StructField(\"runtime\", T.LongType()),\n",
    "        T.StructField(\"season\", T.LongType()),\n",
    "        T.StructField(\"summary\", T.StringType()),\n",
    "        T.StructField(\"url\", T.StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "embedded_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\n",
    "            \"_embedded\",\n",
    "            T.StructType(\n",
    "                [\n",
    "                    T.StructField(\n",
    "                        \"episodes\", T.ArrayType(episode_schema)\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading JSON with stric schema in place\n",
    "\n",
    "This section covers how to read a JSON document while enforcing a precise schema.\n",
    "This proves extremely useful when you want to improve the robustness of your data\n",
    "pipeline; it’s better to know you’re missing a few columns at ingestion time than to get an error later in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "shows_with_schema = spark.read.json(\n",
    "    os.path.join(DIRECTORY,\"shows-silicon-valley.json\"),\n",
    "    schema=embedded_schema,\n",
    "    mode=\"FAILFAST\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|col       |\n",
      "+----------+\n",
      "|2014-04-06|\n",
      "|2014-04-13|\n",
      "|2014-04-20|\n",
      "|2014-04-27|\n",
      "|2014-05-04|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------------+\n",
      "|col                      |\n",
      "+-------------------------+\n",
      "|2014-04-07T02:00:00+00:00|\n",
      "|2014-04-14T02:00:00+00:00|\n",
      "|2014-04-21T02:00:00+00:00|\n",
      "|2014-04-28T02:00:00+00:00|\n",
      "|2014-05-05T02:00:00+00:00|\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in [\"airdate\", \"airstamp\"]:\n",
    "    shows.select(f\"_embedded.episodes.{column}\").select(\n",
    "        F.explode(column)\n",
    "    ).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed\n"
     ]
    }
   ],
   "source": [
    "# witnessing a json document ingestion with incompatible schema\n",
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "episode_schema_BAD = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"_links\", episode_links_schema),\n",
    "        T.StructField(\"airdate\", T.DateType()),\n",
    "        T.StructField(\"airstamp\", T.TimestampType()),\n",
    "        T.StructField(\"airtime\", T.StringType()),\n",
    "        T.StructField(\"id\", T.StringType()),\n",
    "        T.StructField(\"image\", episode_image_schema),\n",
    "        T.StructField(\"name\", T.StringType()),\n",
    "        T.StructField(\"number\", T.LongType()),\n",
    "        T.StructField(\"runtime\", T.LongType()),\n",
    "        T.StructField(\"season\", T.LongType()),\n",
    "        T.StructField(\"summary\", T.LongType()),\n",
    "        T.StructField(\"url\", T.LongType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "embedded_schema2 = T.StructType(\n",
    "    [\n",
    "        T.StructField(\n",
    "            \"_embedded\",\n",
    "            T.StructType(\n",
    "                [\n",
    "                    T.StructField(\n",
    "                        \"episodes\", T.ArrayType(episode_schema_BAD)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "shows_with_schema_wrong = spark.read.json(\n",
    "    \"./data/shows/shows-silicon-valley.json\",\n",
    "    schema=embedded_schema2,\n",
    "    mode=\"FAILFAST\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    shows_with_schema_wrong.show()\n",
    "except Py4JJavaError:\n",
    "    print('failed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going full circle: Specifying the schemas in JSON\n",
    "\n",
    "The StructType object has a handy fromJson() method that will read a JSON-formatted schema. As long as we know how to provide a proper JSON schema, we should be good to go. \n",
    "\n",
    "To understand the layout and content of a typical PySpark data frame, we use our `shows_with_schema` data frame and the schema attribute. Unlike `printSchema()`, which prints our schema to a standard output, schema returns an internal representation of the schema in terms of StructType. Fortunately, StructType comes with two methods for exporting its content into a JSON-esque format:\n",
    "- `json()` will output a string containing the JSON-formatted schema.\n",
    "- `jsonValue()` will return the schema as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': [{'metadata': {},\n",
      "             'name': 'airtime',\n",
      "             'nullable': True,\n",
      "             'type': 'string'}],\n",
      " 'type': 'struct'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(\n",
    "    shows_with_schema.select(\n",
    "        F.explode(\"_embedded.episodes\").alias(\"episode\")\n",
    "    )\n",
    "    .select(\"episode.airtime\")\n",
    "    .schema.jsonValue()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {},\n",
      " 'name': 'array_example',\n",
      " 'nullable': True,\n",
      " 'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(\n",
    "    T.StructField(\"array_example\", T.ArrayType(T.StringType())).jsonValue()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {},\n",
      " 'name': 'map_example',\n",
      " 'nullable': True,\n",
      " 'type': {'keyType': 'string',\n",
      "          'type': 'map',\n",
      "          'valueContainsNull': True,\n",
      "          'valueType': 'long'}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(\n",
    "    T.StructField(\n",
    "    \"map_example\", T.MapType(T.StringType(), T.LongType())\n",
    "    ).jsonValue()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': [{'metadata': {},\n",
      "             'name': 'map_example',\n",
      "             'nullable': True,\n",
      "             'type': {'keyType': 'string',\n",
      "                      'type': 'map',\n",
      "                      'valueContainsNull': True,\n",
      "                      'valueType': 'long'}},\n",
      "            {'metadata': {},\n",
      "             'name': 'array_example',\n",
      "             'nullable': True,\n",
      "             'type': {'containsNull': True,\n",
      "                      'elementType': 'string',\n",
      "                      'type': 'array'}}],\n",
      " 'type': 'struct'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(\n",
    "    T.StructType(\n",
    "        [\n",
    "            T.StructField(\n",
    "                \"map_example\", T.MapType(T.StringType(), T.LongType())\n",
    "            ),\n",
    "            T.StructField(\"array_example\", T.ArrayType(T.StringType())),\n",
    "        ]\n",
    "    ).jsonValue()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "other_shows_schema = T.StructType.fromJson(\n",
    "    json.loads(shows_with_schema.schema.json())\n",
    ")\n",
    "\n",
    "print(other_shows_schema == shows_with_schema.schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together: Reducing duplicate data with complex data types\n",
    "\n",
    "![Data Frame](images/with_json_final.png)\n",
    "\n",
    "Since the beginning of the book, all of our data processing has tried to converge with having a single table. If we want to avoid data duplication, keep the relationship information, and have a single table, then we can—and should!—use the data frame’s complex column types. In our shows data frame\n",
    "- Each record represents a show.\n",
    "- A show has multiple episodes (array of structs column).\n",
    "- Each episode has many fields (struct column within the array).\n",
    "- Each show can have multiple genres (array of string column).\n",
    "- Each show has a schedule (struct column).\n",
    "- Each schedule belonging to a show can have multiple days (array) but a single time (string)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore and Collect\n",
    "\n",
    "how to use explode and collect operations to go from hierarchical to tabular and back. We cover the methods to break an array or a map into discrete records and how to get the records back into the original structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------+\n",
      "| id|                                                              episodes|\n",
      "+---+----------------------------------------------------------------------+\n",
      "|143|{{{http://api.tvmaze.com/episodes/10897}}, 2014-04-06, 2014-04-07T0...|\n",
      "|143|{{{http://api.tvmaze.com/episodes/10898}}, 2014-04-13, 2014-04-14T0...|\n",
      "|143|{{{http://api.tvmaze.com/episodes/10899}}, 2014-04-20, 2014-04-21T0...|\n",
      "|143|{{{http://api.tvmaze.com/episodes/10900}}, 2014-04-27, 2014-04-28T0...|\n",
      "|143|{{{http://api.tvmaze.com/episodes/10901}}, 2014-05-04, 2014-05-05T0...|\n",
      "+---+----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = shows.select(\n",
    "    \"id\", F.explode(\"_embedded.episodes\").alias(\"episodes\")\n",
    ")\n",
    "\n",
    "episodes.show(5, truncate=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode can also happen with maps: the keys and values will be exploded in two different fields. The second type of explosion: posexplode(). The “pos” stands for position: it explodes the column and returns an additional column before the data that contains the position as a long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------------+\n",
      "|position|   id|                name|\n",
      "+--------+-----+--------------------+\n",
      "|       0|10897|Minimum Viable Pr...|\n",
      "|       1|10898|       The Cap Table|\n",
      "|       2|10899|Articles of Incor...|\n",
      "|       3|10900|    Fiduciary Duties|\n",
      "|       4|10901|      Signaling Risk|\n",
      "+--------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episode_name_id = shows.select(\n",
    "    F.map_from_arrays(\n",
    "        F.col(\"_embedded.episodes.id\"), F.col(\"_embedded.episodes.name\")\n",
    "    ).alias(\"name_id\")\n",
    ")\n",
    "\n",
    "episode_name_id = episode_name_id.select(\n",
    "    F.posexplode(\"name_id\").alias(\"position\", \"id\", \"name\")\n",
    ")\n",
    "\n",
    "episode_name_id.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `explode()` and `posexplode()` will skip any null values in the array or the map. If you want to have null as records, you can use `explode_outer()` or `posexplode_outer()` the same way.\n",
    "\n",
    "Now that we have exploded data frames, we’ll do the opposite by collecting our\n",
    "records into a complex column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected = episodes.groupby(\"id\").agg(\n",
    "    F.collect_list(\"episodes\").alias(\"episodes\")\n",
    ")\n",
    "\n",
    "collected.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- episodes: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |-- airstamp: string (nullable = true)\n",
      " |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- number: long (nullable = true)\n",
      " |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |-- season: long (nullable = true)\n",
      " |    |    |-- summary: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collected.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building your won hierarchies: Struct as a function\n",
    "\n",
    "how you can create structs within a data frame. With this last tool in your toolbox, the structure of a data frame will have no secrets for you.\n",
    "\n",
    "To create a struct, we use the `struct()` function from the `pyspark.sql.functions` module. This function takes a number of columns as parameters (just like `select()`) and returns a struct column containing the columns passed as parameters as fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|info             |\n",
      "+-----------------+\n",
      "|{Ended, 96, true}|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "struct_ex = shows.select(\n",
    "    F.struct(\n",
    "        F.col(\"status\"), F.col(\"weight\"), F.lit(True).alias(\"has_watched\")\n",
    "    ).alias(\"info\")\n",
    ")\n",
    "\n",
    "struct_ex.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- info: struct (nullable = false)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- weight: long (nullable = true)\n",
      " |    |-- has_watched: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "struct_ex.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<p style=\"text-align:left;\">\n",
    "    <a href=\"./5_Joining_Grouping.ipynb\">Previous Chapter</a>\n",
    "    <span style=\"float:right;\">\n",
    "        <a href=\"./7_Python_SQL.ipynb\">Next Chapter</a>\n",
    "    </span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
