{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your data under a different lens: Window functions\n",
    "\n",
    "On first glance, they look like a watered-down version of the split-applycombine pattern introduced in previous notebook [Pandas UDFs](./9_Pandas_UDF.ipynb). But it contains powerful manipulations in a short and expressive body of code.\n",
    "\n",
    "Window functions fill a niche between group aggregate (groupBy().agg()) and\n",
    "group map UDF (groupBy().apply()) transformations, both seen in previous notebook [Pandas UDFs](./9_Pandas_UDF.ipynb). Both rely on partitioning to split the data frame based on a predicate. A group aggregate transformation will yield one record per grouping, while a group map UDF allows for any shape of a resulting data frame; a window function always keeps the dimensions of the data frame intact. Window functions have a secret weapon in the window frame that we define within a partition: it determines which records are included in the application of the function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window functions are mostly used for creating new columns, so they leverage\n",
    "some familiar methods, such as select() and withColumn(). But we will see different approach here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growing and using a simle window function\n",
    "\n",
    "For this section, we reuse the temperature data set from [RDD & UDF](./8_RDD_n_UDFs.ipynb); the data set contains weather observations for a series of stations, summarized by day. Window functions especially shine when working with time series-like data (e.g., daily observations of temperature) because you can slice the data by day, month, or year and get useful statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "gsod = spark.read.parquet(\"./data/window/gsod.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the coldest day of each year, the long way\n",
    "\n",
    "In this section, we emulate a simple window function through functionality we learned in previous notebooks using the `join()` method.\n",
    "\n",
    "we start with simple questions to ask our data frame: when and where were the lowest temperature recorded each year? In other words, we want a data frame containing three records, one for each year and showing the station, the date (year, month, day), and the temperature of the coldest day recorded for that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|year|  temp|\n",
      "+----+------+\n",
      "|2019|-114.7|\n",
      "|2017|-114.7|\n",
      "|2018|-113.5|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coldest_temp = gsod.groupby(\"year\").agg(F.min(\"temp\").alias(\"temp\"))\n",
    "coldest_temp.orderBy(\"temp\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the year and the temperature, which are about 40% of the original ask. To get the other three columns (`mo`, `da`, `stn`), we can use a left-semi join on the original table, using the results of `coldest_temp` to resolve the join.\n",
    "\n",
    "we join `gsod` to `coldest_temp` using a left-semi equi-join on the `year` and `temp` columns. Because `coldest_temp` only contains the coldest temperature for each year, the left semi-join keeps only the records from `gsod` that correspond to that year-temperature pair; this is equivalent to keeping only the records where the temperature was coldest for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+------+\n",
      "|   stn|year| mo| da|  temp|\n",
      "+------+----+---+---+------+\n",
      "|896250|2017| 06| 20|-114.7|\n",
      "|896060|2018| 08| 27|-113.5|\n",
      "|895770|2019| 06| 15|-114.7|\n",
      "+------+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coldest_when = gsod.join(\n",
    "    coldest_temp, how=\"left_semi\", on=[\"year\", \"temp\"]\n",
    ").select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "\n",
    "coldest_when.orderBy(\"year\", \"mo\", \"da\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above codes we are performing a join between the gsod table and, well, something coming from the gsod table. A self-join, which is when you join a table with itself, is often considered an anti-pattern for data manipulation. While it’s not technically wrong, it can be slow and make the code look more complex than it needs to be.\n",
    "\n",
    "<img src=\"images/self_join_table.png\">\n",
    "\n",
    "Fortunately, a window function gives you the same result faster, and with less code\n",
    "clutter. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and using a simple window function to get the coldest days\n",
    "\n",
    "We use the `Window` object and parameterize it to split a data frame over column values. We then apply the window over a data frame, using the traditional selector approach.\n",
    "\n",
    "Similar to split-apply-combine pattern we covered in previous notebooks, we will employ three stages\n",
    "- Instead of splitting, we’ll partition the data frame.\n",
    "- Instead of applying, we’ll select values over the window.\n",
    "- The combine/union operation is implicit (i.e., not explicitly coded) in a window function\n",
    "\n",
    "Window functions apply over a window of data split according to the values on a column. Each split, called a partition, gets the window function applied to each of its records as if they were independent data frames. The result then gets unioned back into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.window.WindowSpec object at 0x00000186520A7280>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "each_year = Window.partitionBy(\"year\")\n",
    "\n",
    "print(each_year)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A WindowSpec object is nothing more than a blueprint for an eventual window function. We created a window specification called each_year that instructs the window application to split the data frame according to the values in the year column. The real magic happens when you apply the window function to your data\n",
    "frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+------+\n",
      "|   stn|year| mo| da|  temp|\n",
      "+------+----+---+---+------+\n",
      "|896250|2017| 06| 20|-114.7|\n",
      "|896060|2018| 08| 27|-113.5|\n",
      "|895770|2019| 06| 15|-114.7|\n",
      "+------+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Self-Join Approach (repeat)\n",
    "\n",
    "coldest_when = gsod.join(\n",
    "    coldest_temp, how=\"left_semi\", on=[\"year\", \"temp\"]\n",
    ").select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "\n",
    "coldest_when.orderBy(\"year\", \"mo\", \"da\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the `withColumn()` method we define a column, `min_temp`, that collects the minimum of the `temp` column. Now, rather than picking the minimum temperature of the whole data frame, the `min()` is applied over the window specification we defined, using the `over()` method. For each window partition, Spark computes the minimum and then broadcasts the value over each record.\n",
    "\n",
    "This is an important distinction compared to aggregating functions or UDF: in the\n",
    "case of a window function, *the number of records in the data frame does not change*. Although `min()` is an aggregate function, since it’s applied with the `over()` method, every record in the window has the minimum value appended. The same would apply for any other aggregate function from `pyspark.sql.functions`, such as `sum()`, `avg()`, `min()`, `max()`, and `count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n",
      "|year| mo| da|   stn|  temp|\n",
      "+----+---+---+------+------+\n",
      "|2017| 06| 20|896250|-114.7|\n",
      "|2018| 08| 27|896060|-113.5|\n",
      "|2019| 06| 15|895770|-114.7|\n",
      "+----+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window function\n",
    "\n",
    "(gsod\n",
    ".withColumn(\"min_temp\", F.min(\"temp\").over(each_year))\n",
    ".where(\"temp = min_temp\")\n",
    ".select(\"year\", \"mo\", \"da\", \"stn\", \"temp\")\n",
    ".orderBy(\"year\", \"mo\", \"da\")\n",
    ".show())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Window functions are just methods on columns**\\\n",
    "Since a window function is applied though a method on a `Column` object, you can also apply them in a `select()`. You can also apply more than one window (or different ones) within the same `select()`. Spark won’t allow you to use a window directly in a `groupby()` or `where()` method, where it’ll spit an `AnalysisException`. If you want to group by or filter according to the result of a window function, “materialize” the column using `select()` or `withColumn()` before using the desired operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n",
      "|year| mo| da|   stn|  temp|\n",
      "+----+---+---+------+------+\n",
      "|2017| 06| 20|896250|-114.7|\n",
      "|2018| 08| 27|896060|-113.5|\n",
      "|2019| 06| 15|895770|-114.7|\n",
      "+----+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod.select(\n",
    "    \"year\",\n",
    "    \"mo\",\n",
    "    \"da\",\n",
    "    \"stn\",\n",
    "    \"temp\",\n",
    "    F.min(\"temp\").over(each_year).alias(\"min_temp\"),\n",
    ").where(\n",
    "    \"temp = min_temp\"\n",
    ").drop(\n",
    "    \"min_temp\"\n",
    ").orderBy(\n",
    "    \"year\", \"mo\", \"da\"\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **But data frames already have partitions**\n",
    "Since the beginning of the notebooks, partition has referred to the physical splits of the data on each executor node. Now we are also using partitions with window functions to mean logical splits of the data, which may or may not be equal to the Spark physical ones.\n",
    "\n",
    "<img src=\"images/partitions.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond summarizing: Using ranking and analytical functions\n",
    "\n",
    "There are two families of functions which allow performance of a wider range of operations versus aggregate functions such as `count()`, `sum()`, or `min()`:\n",
    "\n",
    "- The `ranking` family, which provides information about rank (first, second, all\n",
    "the way to last), n-tiles, and the ever so useful row number.\n",
    "- The `analytical` family, which, despite its namesake, covers a variety of behaviors not related to summary or ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|\n",
      "+------+----+---+---+----+----------+\n",
      "|994979|2017| 12| 11|21.3|        21|\n",
      "|998012|2017| 03| 02|31.4|        24|\n",
      "|719200|2017| 10| 09|60.5|        11|\n",
      "|917350|2018| 04| 21|82.6|         9|\n",
      "|076470|2018| 06| 07|65.0|        24|\n",
      "|996470|2018| 03| 12|55.6|        12|\n",
      "|041680|2019| 02| 19|16.1|        15|\n",
      "|949110|2019| 11| 23|54.9|        14|\n",
      "|998252|2019| 04| 18|44.7|        11|\n",
      "|998166|2019| 03| 20|34.8|        12|\n",
      "+------+----+---+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light = spark.read.parquet(\"./data/window/gsod_light.parquet\")\n",
    "gsod_light.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking functions: Quick, who’s first?\n",
    "\n",
    "Ranking functions are used for getting the top (or bottom) record for each window partition, or, more generally, for getting an order according to some column’s value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_per_month_asc = Window.partitionBy(\"mo\").orderBy(\"count_temp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GOLD, SILVER, BRONZE: SIMPLE RANKING USING RANK()\n",
    "\n",
    "With rank(), each record gets a position based on the value contained in one (or more) columns. Identical values have identical ranks—just like medalists in the Olympics, where the same score/time yields the same rank.\n",
    "\n",
    "`rank()` takes no parameters since it ranks according to the `orderBy()` method\n",
    "from the window spec; it would not make sense to order according to one column but\n",
    "rank according to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       3|\n",
      "|917350|2018| 04| 21|82.6|         9|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       1|\n",
      "|949110|2019| 11| 23|54.9|        14|       1|\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.rank().over(temp_per_month_asc)\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `rank()` provides nonconsecutive ranks for each record, based on the\n",
    "value of the ordered value, or the column(s) provided in the `orderBy()` method of\n",
    "the window spec we call. \n",
    "\n",
    "for each window, the lower the count_temp, the lower the rank. When two records have the same ordered value, their ranks are the same. We say that the rank is nonconsecutive because, when you have multiple records that tie for a rank, the next one will be offset by the number of ties. For instance, for `mo` = 03, we have two records with `count_temp` = 12: both are rank 1. The next record (`count_temp` = 24) has a position of 3 rather than 2, because two records tied for the first position."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NO TIES WHEN RANKING: USING DENSE_RANK()\n",
    "\n",
    "What if we want, say, a denser ranking that would allocate consecutive ranks for records? Enter `dense_rank()`. The same principle as rank() applies, where ties share the same rank, but there won’t be any gap between the ranks: 1, 2, 3, and so on. This is practical when you want the second (or third, or any ordinal position) value over a window, rather than the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       2|\n",
      "|917350|2018| 04| 21|82.6|         9|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       1|\n",
      "|949110|2019| 11| 23|54.9|        14|       1|\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.dense_rank().over(temp_per_month_asc)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
