{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending PySpark with Python: RDD & UDFs\n",
    "\n",
    "Instead of using methods provided by `pyspark.sql`, we build our own set of transformations in pure python, using PySpark as a convenient distributing engine. We start with *resilient distributed dataset* (or **RDD**). RDD is like data frame but distributes unordered objects rather than records and columns. RDD is as a bag of elements with no order or relationship to one another. Each element is independent of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "collection = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}]\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "print(collection_rdd)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were trying to store an integer, a string, a floating point number, a tuple, and a dictionary in a single column, the data frame would have (and fail) to find a common denominator to fit those different types of data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating data the RDD way: `map()`, `filter()`, and `reduce()`\n",
    "\n",
    "`map()`, `filter()`, and `reduce()` all take a function (that we will call `f`) as their only parameter and return a copy of the RDD with the desired modifications. We call functions that take other functions as parameters *higher-order functions*. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply one functiono to every object: MAP\n",
    "\n",
    "We start with the most basic and common operation: applying a Python function to\n",
    "every element of the RDD. For this, PySpark provides `map()`. This directly echoes the functionality of the `map()` function in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "def add_one(value):\n",
    "    return value + 1\n",
    "\n",
    "collection_rdd = collection_rdd.map(add_one)\n",
    "\n",
    "try:\n",
    "    print(collection_rdd.collect())\n",
    "except Py4JJavaError:\n",
    "    pass\n",
    "\n",
    "# Stack trace galore! The important bit, you'll get one of the following:\n",
    "# TypeError: can only concatenate str (not \"int\") to str\n",
    "# TypeError: unsupported operand type(s) for +: 'dict' and 'int'\n",
    "# TypeError: can only concatenate tuple (not \"int\") to tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Failure to add_one](./images/rdd_failure_to_add.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 'two', 4.0, ('four', 4), {'five': 5}]\n"
     ]
    }
   ],
   "source": [
    "# improved safer_add_one() function below \n",
    "# which returns the original element if the \n",
    "# function runs into a type error.\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "def safer_add_one(value):\n",
    "    try:\n",
    "        return value + 1\n",
    "    except TypeError:\n",
    "        return value\n",
    "\n",
    "collection_rdd = collection_rdd.map(safer_add_one)\n",
    "\n",
    "print(collection_rdd.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only keep what you want: FILTER\n",
    "\n",
    "`filter()` is used to keep only the element that satisfies a predicate. The RDD version of `filter()` is a little different than the data frame version: it takes a function `f`, which applies to each object (or element) and keeps only those that return a truthful value.\n",
    "\n",
    "The `isinstance()` function returns True if the first argument’s type is present in the second argument; in our case, it’ll test if each element is either a `float` or an `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4.0]\n"
     ]
    }
   ],
   "source": [
    "collection_rdd = collection_rdd.filter(\n",
    "    lambda elem: isinstance(elem, (float, int))\n",
    ")\n",
    "\n",
    "print(collection_rdd.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like `map()`, the function passed as a parameter to `filter()` is applied to every element in the RDD. This time, though, instead of returning the result in a new RDD, we keep the original value if the result of the function is truthy. If the result is falsy, we drop the element."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two elements come in, one comes out: REDUCE\n",
    "\n",
    "This is an important operation of RDD, which enables the summarization of data (similar to `groupby()`/`agg()`) using the data frame. `reduce()`, as its name implies, is used to reduce elements in an RDD.\n",
    "By *reducing*, meaning we are taking two elements and applying a function that will return only one element. PySpark will apply the function to the first two elements, then apply it again to the result and the third element, and so on, until there are no elements left. \n",
    "\n",
    "![Reduce RDD](./images/rdd_reduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "collection_rdd = sc.parallelize([4, 7, 9, 1, 3])\n",
    "\n",
    "print(collection_rdd.reduce(add))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** `reduce()` in a distributed world Because of PySpark’s distributed nature, the data of an RDD can be distributed across multiple partitions. The `reduce()` function will be applied independently on each partition, and then each intermediate value will be sent to the master node for the final reduction. Because of this, you need to provide a commutative and associative function to `reduce()`.  \\\n",
    "\\\n",
    "A *commutative* function is a function where the order in which the arguments are\n",
    "applied is not important. For example, `add()` is commutative, since `a + b = b + a`. Oh the flip side, `subtract()` is not: `a - b != b - a`.  \\\n",
    "\\\n",
    "An *associative* function is a function where how the values are grouped is not important. `add()` is associative, since `(a + b) + c = a + (b + c)`. `subtract()` is not: `(a - b) - c != a - (b - c)`.  \\\n",
    "\\\n",
    "`add()`, `multiply()`, `min()`, and `max()` are both associative and commutative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using python to extend PySpark via UDFs\n",
    "\n",
    "Unlike the RDD, the data frame has a structure enforced by columns. To address this\n",
    "constraint, PySpark provides the possibility of creating UDFs via the `pyspark.sql.functions.udf()` function. What comes in is a regular Python function, and what goes out is a function promoted to work on PySpark columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|fraction|\n",
      "+--------+\n",
      "|[0, 1]  |\n",
      "|[0, 2]  |\n",
      "|[0, 3]  |\n",
      "|[0, 4]  |\n",
      "|[0, 5]  |\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "fractions = [[x,y] for x in range(100) for y in range(1,100)]\n",
    "\n",
    "frac_df = spark.createDataFrame(fractions, [\"numerator\",\"denominator\"])\n",
    "\n",
    "frac_df = frac_df.select(\n",
    "    F.array(F.col(\"numerator\"),F.col(\"denominator\")).alias(\"fraction\")\n",
    ")\n",
    "\n",
    "frac_df.show(5, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using typed Python functions\n",
    "\n",
    "This section covers creating a Python function that will work seamlessly with a PySpark data frame. While Python and Spark usually work seamlessly together, creating and using UDFs requires a few precautions. \n",
    "\n",
    "we will have a function to reduce a fraction and one to transform a fraction into\n",
    "a floating-point number. The blueprint when creating a function destined to become a Python UDF is as follows:\n",
    "1. Create and document the function.\n",
    "2. Make sure the input and output types are compatible.\n",
    "3. Test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "Frac = Tuple[int,int]\n",
    "\n",
    "def py_reduce_fraction(frac: Frac) -> Optional[Frac]:\n",
    "    \"\"\"Reduce a fracction represented as a 2-tuple of integers\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        answer = Fraction(num,denom)\n",
    "        return answer.numerator, answer.denominator\n",
    "    return None\n",
    "\n",
    "assert py_reduce_fraction((3,6)) == (1,2)\n",
    "assert py_reduce_fraction((1,0)) is None\n",
    "\n",
    "def py_fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    \"\"\"Transforms a fraction represented as a 2-tuple of integers into a float.\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None\n",
    "\n",
    "assert py_fraction_to_float((2, 8)) == 0.25\n",
    "assert py_fraction_to_float((10, 0)) is None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is a dynamic language; this means that the type of an object is known at runtime. When working with PySpark’s data frame, where each column has one and only one type, we need to make sure that our UDF will return consistent types. We can use type hints to ensure this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Python functions to UDFs using `udf()`\n",
    "\n",
    "Once you have your Python function created, PySpark provides a simple mechanism\n",
    "to promote to a UDF. This section covers the `udf()` function and how to use it directly to create a UDF, as well as using the decorator to simplify the creation of a UDF.\n",
    "\n",
    "PySpark provides a `udf()` function in the `pyspark.sql.functions` module to promote Python functions to their UDF equivalents. The function takes two parameters:\n",
    "- The function you want to promote\n",
    "- The return type of the generated UDF\n",
    "\n",
    "Below table shows type equivalences between Python and PySpark. If you provide a return type, it must be compatible with the return value of your UDF.\n",
    "\n",
    "| Type Constructor | String representation | Python equivalent                 | \n",
    "|------------------|-----------------------|-----------------------------------|\n",
    "| NullType()       | null                  | None                              |\n",
    "| StringType()     | string                | Python's regular strings          |\n",
    "| BinaryType()     | binary                | bytearray                         |\n",
    "| BooleanType()    | boolean               | bool                              |\n",
    "| DataType()       | date                  | datetime.date(from `datetime` lib)|\n",
    "| TimestampType()  | timestamp             | datetime.datetime                 |\n",
    "| DecimalType(p,s) | decimal               | decimal.Decimal (from the decimal library) |\n",
    "| DoubleType()     | double                | float\n",
    "| FloatType()      | float                 | float*\n",
    "| ByteType()       | byte or tinyint       | int*\n",
    "| IntegerType()    | int                   | int*\n",
    "| LongType()       | long or bigint        | int*\n",
    "| ShortType()      | short or smallint     | int*\n",
    "| ArrayType(T)     | N/A                   | list, tuple, or Numpy array (from the numpy library) |\n",
    "| MapType(K, V)    | N/A                   | dict\n",
    "| StructType([…])  | N/A                   | list or tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We promote the `py_reduce_fraction()` function to a UDF via the `udf()` function. Just like we did with the Python equivalent, we provide a return type to the UDF\n",
    "(this time, an Array of Long, since Array is the companion type of the tuple and Long is the one for Python integers). Once the UDF is created, we can apply it just like any other PySpark function on columns. we chose to create a new column to showcase the before and after; in the sample shown, the fraction appears properly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|fraction|reduced_fraction|\n",
      "+--------+----------------+\n",
      "|[0, 1]  |[0, 1]          |\n",
      "|[0, 2]  |[0, 1]          |\n",
      "|[0, 3]  |[0, 1]          |\n",
      "|[0, 4]  |[0, 1]          |\n",
      "|[0, 5]  |[0, 1]          |\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SparkFrac = T.ArrayType(T.LongType())\n",
    "\n",
    "reduce_fraction = F.udf(py_reduce_fraction, SparkFrac)\n",
    "\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"reduced_fraction\", reduce_fraction(F.col(\"fraction\"))\n",
    ")\n",
    "\n",
    "frac_df.show(5, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also have the option of creating your Python function and promoting it as a UDF\n",
    "using the udf function as a decorator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|reduced_fraction|fraction_float      |\n",
      "+----------------+--------------------+\n",
      "|[3, 50]         |0.06                |\n",
      "|[3, 67]         |0.04477611940298507 |\n",
      "|[3, 76]         |0.039473684210526314|\n",
      "|[2, 85]         |0.023529411764705882|\n",
      "|[4, 15]         |0.26666666666666666 |\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(T.DoubleType())\n",
    "def fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    \"\"\"Transforms a fraction represented as a 2-tuple of integers into a float.\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None\n",
    "\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"fraction_float\", fraction_to_float(F.col(\"reduced_fraction\"))\n",
    ")\n",
    "\n",
    "frac_df.select(\"reduced_fraction\", \"fraction_float\").distinct().show(5, False)\n",
    "\n",
    "assert fraction_to_float.func((1, 2)) == 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<p style=\"text-align:left;\">\n",
    "    <a href=\"./7_Python_SQL.ipynb\">Previous Chapter</a>\n",
    "    <span style=\"float:right;\">\n",
    "        <a href=\"./9_Pandas_UDF.ipynb\">Next Chapter</a>\n",
    "    </span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
