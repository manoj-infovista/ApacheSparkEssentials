{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending PySpark with Python: RDD & UDFs\n",
    "\n",
    "Instead of using methods provided by `pyspark.sql`, we build our own set of transformations in pure python, using PySpark as a convenient distributing engine. We start with *resilient distributed dataset* (or **RDD**). RDD is like data frame but distributes unordered objects rather than records and columns. RDD is as a bag of elements with no order or relationship to one another. Each element is independent of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "# !set PYSPARK_PYTHON=%cd%\\venv\\Scripts\\python.exe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "collection = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}]\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "print(collection_rdd)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were trying to store an integer, a string, a floating point number, a tuple, and a dictionary in a single column, the data frame would have (and fail) to find a common denominator to fit those different types of data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating data the RDD way: `map()`, `filter()`, and `reduce()`\n",
    "\n",
    "`map()`, `filter()`, and `reduce()` all take a function (that we will call `f`) as their only parameter and return a copy of the RDD with the desired modifications. We call functions that take other functions as parameters *higher-order functions*. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply one functiono to every object: MAP\n",
    "\n",
    "We start with the most basic and common operation: applying a Python function to\n",
    "every element of the RDD. For this, PySpark provides `map()`. This directly echoes the functionality of the `map()` function in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "def add_one(value):\n",
    "    return value + 1\n",
    "\n",
    "collection_rdd = collection_rdd.map(add_one)\n",
    "\n",
    "try:\n",
    "    print(collection_rdd.collect())\n",
    "except Py4JJavaError:\n",
    "    pass\n",
    "\n",
    "# Stack trace galore! The important bit, you'll get one of the following:\n",
    "# TypeError: can only concatenate str (not \"int\") to str\n",
    "# TypeError: unsupported operand type(s) for +: 'dict' and 'int'\n",
    "# TypeError: can only concatenate tuple (not \"int\") to tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Failure to add_one](./images/rdd_failure_to_add.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 'two', 4.0, ('four', 4), {'five': 5}]\n"
     ]
    }
   ],
   "source": [
    "# improved safer_add_one() function below \n",
    "# which returns the original element if the \n",
    "# function runs into a type error.\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "def safer_add_one(value):\n",
    "    try:\n",
    "        return value + 1\n",
    "    except TypeError:\n",
    "        return value\n",
    "\n",
    "collection_rdd = collection_rdd.map(safer_add_one)\n",
    "\n",
    "print(collection_rdd.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only keep what you want: FILTER\n",
    "\n",
    "`filter()` is used to keep only the element that satisfies a predicate. The RDD version of `filter()` is a little different than the data frame version: it takes a function `f`, which applies to each object (or element) and keeps only those that return a truthful value.\n",
    "\n",
    "The `isinstance()` function returns True if the first argument’s type is present in the second argument; in our case, it’ll test if each element is either a `float` or an `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4.0]\n"
     ]
    }
   ],
   "source": [
    "collection_rdd = collection_rdd.filter(\n",
    "    lambda elem: isinstance(elem, (float, int))\n",
    ")\n",
    "\n",
    "print(collection_rdd.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like `map()`, the function passed as a parameter to `filter()` is applied to every element in the RDD. This time, though, instead of returning the result in a new RDD, we keep the original value if the result of the function is truthy. If the result is falsy, we drop the element."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two elements come in, one comes out: REDUCE\n",
    "\n",
    "This is an important operation of RDD, which enables the summarization of data (similar to `groupby()`/`agg()`) using the data frame. `reduce()`, as its name implies, is used to reduce elements in an RDD.\n",
    "By *reducing*, meaning we are taking two elements and applying a function that will return only one element. PySpark will apply the function to the first two elements, then apply it again to the result and the third element, and so on, until there are no elements left. \n",
    "\n",
    "![Reduce RDD](./images/rdd_reduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "collection_rdd = sc.parallelize([4, 7, 9, 1, 3])\n",
    "\n",
    "print(collection_rdd.reduce(add))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** `reduce()` in a distributed world Because of PySpark’s distributed nature, the data of an RDD can be distributed across multiple partitions. The `reduce()` function will be applied independently on each partition, and then each intermediate value will be sent to the master node for the final reduction. Because of this, you need to provide a commutative and associative function to `reduce()`.  \\\n",
    "\\\n",
    "A *commutative* function is a function where the order in which the arguments are\n",
    "applied is not important. For example, `add()` is commutative, since `a + b = b + a`. Oh the flip side, `subtract()` is not: `a - b != b - a`.  \\\n",
    "\\\n",
    "An *associative* function is a function where how the values are grouped is not important. `add()` is associative, since `(a + b) + c = a + (b + c)`. `subtract()` is not: `(a - b) - c != a - (b - c)`.  \\\n",
    "\\\n",
    "`add()`, `multiply()`, `min()`, and `max()` are both associative and commutative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using python to extend PySpark via UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
