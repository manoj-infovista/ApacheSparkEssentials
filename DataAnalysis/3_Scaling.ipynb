{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting and Scaling the PySpark Program\n",
    "\n",
    "we will continue on the project that we started in our [previous notebook](2_First_Steps.ipynb)\n",
    "\n",
    "Steps to carry out:\n",
    "1. [*Read*](./2_First_Steps.ipynb#Step-1-READ)—Read the input data (we’re assuming a plain text file).\n",
    "2. [*Token*](./2_First_Steps.ipynb#Step-2-Token)—Tokenize each word.\n",
    "3. [*Clean*](./2_First_Steps.ipynb#Step-3-Clean)—Remove any punctuation and/or tokens that aren’t words. Lowercase each word.\n",
    "4. [*Count*](#Step-4-Count)—Count the frequency of each word present in the text.\n",
    "5. [*Answer*](#step-5)—Return the top 10 (or 20, 50, 100)\n",
    "\n",
    "![A simple program](images/first_steps_simple_program.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring our code and variable\n",
    "# to where we left off in previous notebook\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, lower, regexp_extract\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
    ").getOrCreate()\n",
    "\n",
    "book = spark.read.text(\"data/gutenberg_books/1342-0.txt\")\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word\")\n",
    ")\n",
    "\n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-4\"></a>\n",
    "## Step-4-Count\n",
    "\n",
    "This section shows you how to count records using the GroupedData object and perform an aggregation function—here, counting the items—on each group. we count the number of each word by creating groups: one for each word. Once those groups are formed, we can perform an aggregation function on each one of them. In this specific case, we count the number of records for each group, which will give us the number of occurrences for each word in the data frame. \n",
    "\n",
    "![Grouping words](images/scaling_grouping.png)\n",
    "\n",
    "The easiest way to count record occurrence is to use the groupby() method, passing\n",
    "the columns we wish to group as a parameter. The groupby() method returns a GroupedData and awaits further instructions. Once we apply the count() method, we get back a data frame containing the grouping column word, as well as the count column containing the number of occurrences for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.group.GroupedData object at 0x000001A05F179520>\n"
     ]
    }
   ],
   "source": [
    "groups = words_nonull.groupby(col(\"word\"))\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[word: string, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "results = words_nonull.groupby(col(\"word\")).count()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         word|count|\n",
      "+-------------+-----+\n",
      "|       online|    4|\n",
      "|         some|  203|\n",
      "|        still|   72|\n",
      "|          few|   72|\n",
      "|         hope|  122|\n",
      "|        those|   60|\n",
      "|     cautious|    4|\n",
      "|    imitation|    1|\n",
      "|          art|    3|\n",
      "|      solaced|    1|\n",
      "|       poetry|    2|\n",
      "|    arguments|    5|\n",
      "| premeditated|    1|\n",
      "|      elevate|    1|\n",
      "|       doubts|    2|\n",
      "|    destitute|    1|\n",
      "|    solemnity|    5|\n",
      "|   lieutenant|    1|\n",
      "|gratification|    1|\n",
      "|    connected|   14|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-5\"></a>\n",
    "## Step 5 Answer\n",
    "\n",
    "we order the data to find the top 10 words with highest occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1750|\n",
      "| she| 1668|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.orderBy(col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the result to csv\n",
    "\n",
    "# writing the result as multiple partitions.\n",
    "# results.write.mode(\"overwrite\").csv(\"data/gutenberg_books/sample_ouput.csv\")\n",
    "\n",
    "# to write into a single file\n",
    "results.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").format(\"csv\").save(\n",
    "    \"data/gutenberg_books/sample_ouput.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entire code \n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,explode,lower,regexp_extract,split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Analyzing the vocabulary of Pride and Prejudice.\").getOrCreate()\n",
    "\n",
    "book = spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "                            regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
    "                            )\n",
    "\n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
    "\n",
    "results = words_nonull.groupby(col(\"word\")).count()\n",
    "\n",
    "results.orderBy(\"count\", ascending=False).show(10)\n",
    "\n",
    "results.coalesce(1).write.csv(\"./simple_count_single_partition.csv\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simplifying the code via method chaining\n",
    "\n",
    "![Method chaining](images/scaling_method_chaining.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "results = (\n",
    "    spark.read.text(\"data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(\"word\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1749|\n",
      "| she| 1668|\n",
      "|that| 1487|\n",
      "|  it| 1482|\n",
      "| not| 1427|\n",
      "| you| 1300|\n",
      "|  he| 1296|\n",
      "|  be| 1257|\n",
      "| his| 1247|\n",
      "|  as| 1174|\n",
      "| had| 1170|\n",
      "|with| 1092|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
